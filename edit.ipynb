{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"\n",
    "ファイルの編集を色々とするためのファイル\n",
    "\"\"\"\n",
    "from filer2.filer2 import Filer\n",
    "import numpy as np\n",
    "import glob\n",
    "import collections\n",
    "import re\n",
    "import MeCab\n",
    "import itertools\n",
    "import CaboCha\n",
    "from stanford_corenlp_pywrapper import CoreNLP\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CoreNLP_PyWrapper:Starting java subprocess, and waiting for signal it's ready, with command: exec java -Xmx4g -XX:ParallelGCThreads=1 -cp '/home/ikegami/ikegami/lib/python2.7/site-packages/stanford_corenlp_pywrapper/lib/*:/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*'      corenlp.SocketServer --outpipe /tmp/corenlp_pywrap_pipe_pypid=12464_time=1475136941.89  --configdict '{\"annotators\": \"tokenize, ssplit, pos, lemma, parse\"}'\n",
      "INFO:CoreNLP_PyWrapper:Successful ping. The server has started.\n",
      "INFO:CoreNLP_PyWrapper:Subprocess is ready.\n"
     ]
    }
   ],
   "source": [
    "# 関数の定義\n",
    "mecab = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "proc = CoreNLP(configdict={'annotators': 'tokenize, ssplit, pos, lemma, parse'},\n",
    "               corenlp_jars=[\"/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*\"])\n",
    "\n",
    "def morpho(sentence):\n",
    "    res = mecab.parseToNode(sentence)\n",
    "    list_words = []\n",
    "    while res:\n",
    "        features = res.feature.split(\",\")\n",
    "        if (features[0] == \"名詞\" and features[1] in [\"一般\", \"固有名詞\", \"サ変接続\", \"形容動詞語幹\"]) or features[0] == \"形容詞\":\n",
    "            if features[6] == \"*\":\n",
    "                if res.surface != '':\n",
    "                    list_words.append(res.surface)\n",
    "            else:\n",
    "                if features[6] != '':\n",
    "                    list_words.append(features[6])\n",
    "        res = res.next\n",
    "    return list_words\n",
    "\n",
    "def morpho_v(sentence):\n",
    "    res = mecab.parseToNode(sentence)\n",
    "    list_words = []\n",
    "    while res:\n",
    "        features = res.feature.split(\",\")\n",
    "        if (features[0] == \"名詞\" and features[1] in [\"一般\", \"固有名詞\", \"サ変接続\", \"形容動詞語幹\"]) or features[0] == \"形容詞\" or (features[0] == \"動詞\" and features[1] == \"自立\"):\n",
    "            if features[6] == \"*\":\n",
    "                if res.surface != '':\n",
    "                    list_words.append(res.surface)\n",
    "            else:\n",
    "                if features[6] != '':\n",
    "                    list_words.append(features[6])\n",
    "        res = res.next\n",
    "    return list_words\n",
    "\n",
    "def morpho1(sentence):\n",
    "    res = mecab.parseToNode(sentence)\n",
    "    list_words = []\n",
    "    while res:\n",
    "        features = res.feature.split(\",\")\n",
    "        if features[0] != \"記号\":\n",
    "            if features[6] == \"*\":\n",
    "                if res.surface != '':\n",
    "                    list_words.append(res.surface)\n",
    "            else:\n",
    "                if features[6] != '':\n",
    "                    list_words.append(features[6])\n",
    "        res = res.next\n",
    "    return list_words\n",
    "\n",
    "cp = CaboCha.Parser('-f1 -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "\n",
    "def get_v_word(tree, chunk):\n",
    "    surface = []\n",
    "    for i in range(chunk.token_pos, chunk.token_pos + chunk.token_size):\n",
    "        token = tree.token(i)\n",
    "        features = token.feature.split(',')\n",
    "        if features[0] == '名詞' and features[1] in [\"一般\", \"固有名詞\", \"サ変接続\", \"形容動詞語幹\"]:\n",
    "            surface.append(token.surface)\n",
    "        elif features[0] == '形容詞':\n",
    "            surface.append(features[6])\n",
    "        elif features[0] == '動詞' and features[1] == '自立':\n",
    "            surface.append(features[6])\n",
    "    return tuple(surface)\n",
    "\n",
    "def get_v_words(line):\n",
    "    cp = CaboCha.Parser('-f1 -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "    tree = cp.parse(line)\n",
    "    chunk_dic = {}\n",
    "    chunk_id = 0\n",
    "    for i in range(0, tree.size()):\n",
    "        token = tree.token(i)\n",
    "        if token.chunk:\n",
    "            chunk_dic[chunk_id] = token.chunk\n",
    "            chunk_id += 1\n",
    "\n",
    "    tuples = []\n",
    "    for chunk_id, chunk in chunk_dic.items():\n",
    "        if chunk.link > 0:\n",
    "            from_surface =  get_v_word(tree, chunk)\n",
    "            to_chunk = chunk_dic[chunk.link]\n",
    "            to_surface = get_v_word(tree, to_chunk)\n",
    "            tuples.extend(itertools.product(from_surface, to_surface))\n",
    "    return tuples\n",
    "\n",
    "def get_word(tree, chunk):\n",
    "    surface = []\n",
    "    for i in range(chunk.token_pos, chunk.token_pos + chunk.token_size):\n",
    "        token = tree.token(i)\n",
    "        features = token.feature.split(',')\n",
    "        if features[0] == '名詞' and features[1] in [\"一般\", \"固有名詞\", \"サ変接続\", \"形容動詞語幹\"]:\n",
    "            surface.append(token.surface)\n",
    "        elif features[0] == '形容詞':\n",
    "            surface.append(features[6])\n",
    "    return tuple(surface)\n",
    "\n",
    "def get_words(line):\n",
    "    cp = CaboCha.Parser('-f1 -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "    tree = cp.parse(line)\n",
    "    chunk_dic = {}\n",
    "    chunk_id = 0\n",
    "    for i in range(0, tree.size()):\n",
    "        token = tree.token(i)\n",
    "        if token.chunk:\n",
    "            chunk_dic[chunk_id] = token.chunk\n",
    "            chunk_id += 1\n",
    "\n",
    "    tuples = []\n",
    "    for chunk_id, chunk in chunk_dic.items():\n",
    "        if chunk.link > 0:\n",
    "            from_surface =  get_word(tree, chunk)\n",
    "            to_chunk = chunk_dic[chunk.link]\n",
    "            to_surface = get_word(tree, to_chunk)\n",
    "            tuples.extend(itertools.product(from_surface, to_surface))\n",
    "    return tuples\n",
    "\n",
    "p_n = r'^NN'\n",
    "p_v = r'^VB'\n",
    "p_j = r'^JJ'\n",
    "\n",
    "def morpho_e_njv(sentence):\n",
    "    list_sentence = proc.parse_doc(sentence)['sentences']\n",
    "    list_all_bag = []\n",
    "    list_all_edge = []\n",
    "    list_all_sentence = []\n",
    "    list_all_lemmas = []\n",
    "    for dict_tmp in list_sentence:\n",
    "        sentence = [word for word in dict_tmp['tokens']\n",
    "                    if word != ',' and word != '.' and word != ' ']\n",
    "        list_all_sentence.append(' '.join(sentence))\n",
    "        list_lemmas = dict_tmp['tokens']\n",
    "        list_deps = dict_tmp['deps_cc']\n",
    "        list_pos = dict_tmp['pos']\n",
    "        dict_key_lemmas = {}\n",
    "        dict_lemmas_key = {}\n",
    "        list_bag = []\n",
    "        for i, row in enumerate(zip(list_lemmas, list_pos)):\n",
    "            lemmas = row[0]\n",
    "            pos = row[1]\n",
    "\n",
    "            if re.match(p_n, pos) or re.match(p_j, pos) or re.match(p_v, pos):\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "            \n",
    "        list_edge = []\n",
    "        for deps in list_deps:\n",
    "            if deps[1] in dict_key_lemmas and deps[2] in dict_key_lemmas:\n",
    "                list_edge.append([dict_key_lemmas[deps[1]],\n",
    "                                  dict_key_lemmas[deps[2]]])\n",
    "\n",
    "        list_all_bag.append(list_bag)\n",
    "        list_all_edge.append(list_edge)\n",
    "    return list_all_sentence, list_all_bag, list_all_edge\n",
    "\n",
    "def morpho_e(sentence):\n",
    "    list_sentence = proc.parse_doc(sentence)['sentences']\n",
    "    list_all_bag = []\n",
    "    list_all_edge = []\n",
    "    list_all_sentence = []\n",
    "    for dict_tmp in list_sentence:\n",
    "        sentence = [word for word in dict_tmp['tokens']\n",
    "                    if word != ',' and word != '.' and word != ' ']\n",
    "        list_all_sentence.append(' '.join(sentence))\n",
    "        list_lemmas = dict_tmp['tokens']\n",
    "        list_deps = dict_tmp['deps_cc']\n",
    "        list_pos = dict_tmp['pos']\n",
    "        dict_key_lemmas = {}\n",
    "        dict_lemmas_key = {}\n",
    "        list_bag = []\n",
    "        for i, row in enumerate(zip(list_lemmas, list_pos)):\n",
    "            lemmas = row[0]\n",
    "            pos = row[1]\n",
    "\n",
    "            if lemmas != ',' and lemmas != ' ' and lemmas != '.':\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "            \n",
    "        list_edge = []\n",
    "        for deps in list_deps:\n",
    "            if deps[1] in dict_key_lemmas and deps[2] in dict_key_lemmas:\n",
    "                list_edge.append([dict_key_lemmas[deps[1]],\n",
    "                                  dict_key_lemmas[deps[2]]])\n",
    "\n",
    "        list_all_bag.append(list_bag)\n",
    "        list_all_edge.append(list_edge)\n",
    "    return list_all_sentence, list_all_bag, list_all_edge\n",
    "\n",
    "def morpho_e_nj(sentence):\n",
    "    list_sentence = proc.parse_doc(sentence)['sentences']\n",
    "    list_all_bag = []\n",
    "    list_all_edge = []\n",
    "    list_all_sentence = []\n",
    "    for dict_tmp in list_sentence:\n",
    "        sentence = [word for word in dict_tmp['tokens']\n",
    "                    if word != ',' and word != '.' and word != ' ']\n",
    "        list_all_sentence.append(' '.join(sentence))\n",
    "        list_lemmas = dict_tmp['tokens']\n",
    "        list_deps = dict_tmp['deps_cc']\n",
    "        list_pos = dict_tmp['pos']\n",
    "        dict_key_lemmas = {}\n",
    "        dict_lemmas_key = {}\n",
    "        list_bag = []\n",
    "        for i, row in enumerate(zip(list_lemmas, list_pos)):\n",
    "            lemmas = row[0]\n",
    "            pos = row[1]\n",
    "\n",
    "            if re.match(p_n, pos) or re.match(p_j, pos):\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "            \n",
    "        list_edge = []\n",
    "        for deps in list_deps:\n",
    "            if deps[1] in dict_key_lemmas and deps[2] in dict_key_lemmas:\n",
    "                list_edge.append([dict_key_lemmas[deps[1]],\n",
    "                                  dict_key_lemmas[deps[2]]])\n",
    "\n",
    "        list_all_bag.append(list_bag)\n",
    "        list_all_edge.append(list_edge)\n",
    "    return list_all_sentence, list_all_bag, list_all_edge\n",
    "\n",
    "\n",
    "def morpho_e_njv_pos(sentence):\n",
    "    list_sentence = proc.parse_doc(sentence)['sentences']\n",
    "    list_all_bag = []\n",
    "    list_all_edge = []\n",
    "    list_all_sentence = []\n",
    "    list_all_lemmas = []\n",
    "    list_all_pos = []\n",
    "    for dict_tmp in list_sentence:\n",
    "        sentence = [word for word in dict_tmp['tokens']\n",
    "                    if word != ',' and word != '.' and word != ' ']\n",
    "        list_all_sentence.append(' '.join(sentence))\n",
    "        list_lemmas = dict_tmp['tokens']\n",
    "        list_deps = dict_tmp['deps_cc']\n",
    "        list_pos = dict_tmp['pos']\n",
    "        dict_key_lemmas = {}\n",
    "        dict_lemmas_key = {}\n",
    "        list_bag = []\n",
    "        list_p = []\n",
    "        for i, row in enumerate(zip(list_lemmas, list_pos)):\n",
    "            lemmas = row[0]\n",
    "            pos = row[1]\n",
    "\n",
    "            if re.match(p_n, pos):\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "                list_p.append('n')\n",
    "            elif re.match(p_j, pos):\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "                list_p.append('j')\n",
    "            elif re.match(p_v, pos):\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "                list_p.append('v')\n",
    "            \n",
    "        list_edge = []\n",
    "        for deps in list_deps:\n",
    "            if deps[1] in dict_key_lemmas and deps[2] in dict_key_lemmas:\n",
    "                list_edge.append([dict_key_lemmas[deps[1]],\n",
    "                                  dict_key_lemmas[deps[2]]])\n",
    "\n",
    "        list_all_bag.append(list_bag)\n",
    "        list_all_edge.append(list_edge)\n",
    "        list_all_pos.append(list_p)\n",
    "    return list_all_sentence, list_all_bag, list_all_edge, list_all_pos\n",
    "\n",
    "def morpho_e_pos(sentence):\n",
    "    list_sentence = proc.parse_doc(sentence)['sentences']\n",
    "    list_all_bag = []\n",
    "    list_all_edge = []\n",
    "    list_all_sentence = []\n",
    "    list_all_pos = []\n",
    "    for dict_tmp in list_sentence:\n",
    "        sentence = [word for word in dict_tmp['tokens']\n",
    "                    if word != ',' and word != '.' and word != ' ']\n",
    "        list_all_sentence.append(' '.join(sentence))\n",
    "        list_lemmas = dict_tmp['tokens']\n",
    "        list_deps = dict_tmp['deps_cc']\n",
    "        list_pos = dict_tmp['pos']\n",
    "        dict_key_lemmas = {}\n",
    "        dict_lemmas_key = {}\n",
    "        list_bag = []\n",
    "        list_p = []\n",
    "        for i, row in enumerate(zip(list_lemmas, list_pos)):\n",
    "            lemmas = row[0]\n",
    "            pos = row[1]\n",
    "\n",
    "            if lemmas != ',' and lemmas != ' ' and lemmas != '.':\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "                if re.match(p_n, pos):\n",
    "                    list_p.append('n')\n",
    "                elif re.match(p_j, pos):\n",
    "                    list_p.append('j')\n",
    "                elif re.match(p_v, pos):\n",
    "                    list_p.append('v')\n",
    "                else:\n",
    "                    list_p.append('-')\n",
    "            \n",
    "        list_edge = []\n",
    "        for deps in list_deps:\n",
    "            if deps[1] in dict_key_lemmas and deps[2] in dict_key_lemmas:\n",
    "                list_edge.append([dict_key_lemmas[deps[1]],\n",
    "                                  dict_key_lemmas[deps[2]]])\n",
    "\n",
    "        list_all_bag.append(list_bag)\n",
    "        list_all_edge.append(list_edge)\n",
    "        list_all_pos.append(list_p)\n",
    "    return list_all_sentence, list_all_bag, list_all_edge, list_all_pos\n",
    "\n",
    "def morpho_e_nj_pos(sentence):\n",
    "    list_sentence = proc.parse_doc(sentence)['sentences']\n",
    "    list_all_bag = []\n",
    "    list_all_edge = []\n",
    "    list_all_sentence = []\n",
    "    list_all_pos = []\n",
    "    for dict_tmp in list_sentence:\n",
    "        sentence = [word for word in dict_tmp['tokens']\n",
    "                    if word != ',' and word != '.' and word != ' ']\n",
    "        list_all_sentence.append(' '.join(sentence))\n",
    "        list_lemmas = dict_tmp['tokens']\n",
    "        list_deps = dict_tmp['deps_cc']\n",
    "        list_pos = dict_tmp['pos']\n",
    "        dict_key_lemmas = {}\n",
    "        dict_lemmas_key = {}\n",
    "        list_bag = []\n",
    "        list_p = []\n",
    "        for i, row in enumerate(zip(list_lemmas, list_pos)):\n",
    "            lemmas = row[0]\n",
    "            pos = row[1]\n",
    "\n",
    "            if re.match(p_n, pos):\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "                list_p.append('n')\n",
    "            elif re.match(p_j, pos):\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "                list_p.append('j')\n",
    "            \n",
    "        list_edge = []\n",
    "        for deps in list_deps:\n",
    "            if deps[1] in dict_key_lemmas and deps[2] in dict_key_lemmas:\n",
    "                list_edge.append([dict_key_lemmas[deps[1]],\n",
    "                                  dict_key_lemmas[deps[2]]])\n",
    "\n",
    "        list_all_bag.append(list_bag)\n",
    "        list_all_edge.append(list_edge)\n",
    "        list_all_pos.append(list_p)\n",
    "    return list_all_sentence, list_all_bag, list_all_edge, list_all_pos\n",
    "\n",
    "def morpho_j(sentence):\n",
    "    res = mecab.parseToNode(sentence)\n",
    "    list_words = []\n",
    "    list_all_word = []\n",
    "    list_all_lemmas = []\n",
    "    list_lemmas = []\n",
    "    while res:\n",
    "        features = res.feature.split(\",\")\n",
    "        pos = features[0]\n",
    "        subpos = features[1]\n",
    "        tokens = res.surface\n",
    "        lemmas = features[6]\n",
    "        if subpos != \"読点\" and subpos != \"句点\" and tokens != '':\n",
    "            list_all_word.append(tokens)\n",
    "            if lemmas != '*' and pos != '名詞':\n",
    "                list_all_lemmas.append(lemmas)\n",
    "            else:\n",
    "                list_all_lemmas.append(tokens)\n",
    "        if (pos == \"名詞\" and subpos in [\"一般\", \"固有名詞\", \"サ変接続\", \"形容動詞語幹\"]) or pos == \"形容詞\":\n",
    "            list_words.append(tokens)\n",
    "            if lemmas != '*' and pos != '名詞':\n",
    "                list_lemmas.append(lemmas)\n",
    "            else:\n",
    "                list_lemmas.append(tokens)\n",
    "        res = res.next\n",
    "    return list_all_word, list_words, list_all_lemmas, list_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_sentence, list_lemmas, list_bag, list_edge = morpho_e('The clusters of review sentences on the viewpoints from the products evaluation can be applied to various use. The topic models, for example Unigram Mixture (UM), can be used for this task.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'The', u'clusters', u'of', u'review', u'sentences', u'on', u'the', u'viewpoints', u'from', u'the', u'products', u'evaluation', u'can', u'be', u'applied', u'to', u'various', u'use'] [u'the', u'cluster', u'of', u'review', u'sentence', u'on', u'the', u'viewpoint', u'from', u'the', u'product', u'evaluation', u'can', u'be', u'apply', u'to', u'various', u'use'] ['cluster', 'review', 'sentence', 'viewpoint', 'product', 'evaluation', 'various', 'use'] [['cluster', 'sentence'], ['sentence', 'review'], ['sentence', 'viewpoint'], ['viewpoint', 'evaluation'], ['evaluation', 'product'], ['use', 'various']]\n",
      "[u'The', u'topic', u'models', u'for', u'example', u'Unigram', u'Mixture', u'-LRB-', u'UM', u'-RRB-', u'can', u'be', u'used', u'for', u'this', u'task'] [u'the', u'topic', u'model', u'for', u'example', u'Unigram', u'Mixture', u'-lrb-', u'UM', u'-rrb-', u'can', u'be', u'use', u'for', u'this', u'task'] ['topic', 'model', 'example', 'Unigram', 'Mixture', 'UM', 'task'] [['model', 'Mixture'], ['model', 'topic'], ['Mixture', 'example'], ['Mixture', 'Unigram'], ['Mixture', 'UM']]\n"
     ]
    }
   ],
   "source": [
    "for sentence, lemmas, bag, edge in zip(list_sentence, list_lemmas, list_bag, list_edge):\n",
    "    print sentence, lemmas, bag, edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全体のコーパスを施設別に集計する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = glob.glob('../topic_model/files/rakuten_corpus/rakuten_corpus_master/txtfile/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "list_file = [Filer.readtxt(path)  for path in list_filepath]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# タブで区切る\n",
    "list_file = [[sentence.split('\\t') for sentence in row] for row in list_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ホテル番号別に集計\n",
    "dict_hotel_sentence = collections.defaultdict(list)\n",
    "for row in list_file:\n",
    "    for sen in row:\n",
    "        dict_hotel_sentence[sen[0]].append(sen[1] + '\\t' + sen[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# review数が100件未満のホテルは除外して保存する\n",
    "for key, value in dict_hotel_sentence.items():\n",
    "    if len(value) >= 100:\n",
    "        Filer.writetxt(value, '../files/txtfile/review-'+key+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数を集計\n",
    "dict_count = {key: len(value) for key, value in dict_hotel_sentence.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 施設名と番号を合わせる\n",
    "list_name = Filer.readtxt('../files/travel03_hotelMaster_20160304.txt')\n",
    "list_name = [sen.split('\\t') for sen in list_name]\n",
    "dict_num_name = {row[0]: row[1] for row in list_name if len(row) == 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 年別にレビュー数を集計\n",
    "dict_counter = {}\n",
    "for key, value in dict_hotel_sentence.items():\n",
    "    dict_counter_tmp = collections.defaultdict(int)\n",
    "    for sen in value:\n",
    "        dict_counter_tmp[sen[0:4]] += 1\n",
    "    dict_counter[key] = dict_counter_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'2001': 3,\n",
       "             '2002': 6,\n",
       "             '2003': 17,\n",
       "             '2004': 6,\n",
       "             '2005': 54,\n",
       "             '2006': 288,\n",
       "             '2007': 96,\n",
       "             '2008': 42,\n",
       "             '2009': 38,\n",
       "             '2010': 31,\n",
       "             '2011': 11})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_count['9680']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 施設番号、施設名、レビュー数のファイルを作成する\n",
    "list_master = [['施設番号', '施設名', 'レビュー数',\n",
    "                '1999', '2000', '2001', '2002',\n",
    "                '2003', '2004', '2005', '2006',\n",
    "                '2007', '2008', '2009', '2010',\n",
    "                '2011']]\n",
    "for key, value in sorted(dict_count.items(), key=lambda x:x[1], reverse=True):\n",
    "    if key in dict_num_name:\n",
    "        list_master.append([key, dict_num_name[key], value, dict_counter[key]['1999'],\n",
    "                            dict_counter[key]['2000'], dict_counter[key]['2001'], dict_counter[key]['2002'],\n",
    "                            dict_counter[key]['2003'], dict_counter[key]['2004'], dict_counter[key]['2005'],\n",
    "                            dict_counter[key]['2006'], dict_counter[key]['2007'], dict_counter[key]['2008'],\n",
    "                            dict_counter[key]['2009'], dict_counter[key]['2010'], dict_counter[key]['2011']])\n",
    "    else:\n",
    "        list_master.append([key, '', value, dict_counter[key]['1999'],\n",
    "                            dict_counter[key]['2000'], dict_counter[key]['2001'], dict_counter[key]['2002'],\n",
    "                            dict_counter[key]['2003'], dict_counter[key]['2004'], dict_counter[key]['2005'],\n",
    "                            dict_counter[key]['2006'], dict_counter[key]['2007'], dict_counter[key]['2008'],\n",
    "                            dict_counter[key]['2009'], dict_counter[key]['2010'], dict_counter[key]['2011']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# csvfileとして保存\n",
    "Filer.writecsv(list_master, '../files/txtfile/master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析対象とするホテルのみを取り出して、sentenceごとのファイルにする\n",
    "* 51637：アパホテル＆リゾート＜東京ベイ幕張＞\n",
    "* 1238：オリエンタルホテル東京ベイ\n",
    "* 53171：加賀の湧泉　ドーミーイン金沢\n",
    "* 19191：新横浜プリンスホテル\n",
    "* 76887：ホテルモントレ　グラスミア大阪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = ['../files/txtfile/review-51637.txt',\n",
    "                 '../files/txtfile/review-1238.txt',\n",
    "                 '../files/txtfile/review-53171.txt',\n",
    "                 '../files/txtfile/review-19191.txt',\n",
    "                 '../files/txtfile/review-76887.txt',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2010年、2011年の文章のみを取得\n",
    "list_review_rev = []\n",
    "for path in list_filepath:\n",
    "    list_review = Filer.readtxt(path)\n",
    "    list_tmp = []\n",
    "    for review in list_review:\n",
    "        if review[0:4] == '2010' or review[0:4] == '2011':\n",
    "            list_tmp.append(review.split('\\t')[1])\n",
    "    list_review_rev.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#【ご利用の宿泊プラン】~を削除\n",
    "pattern = r'【ご利用の宿泊プラン】.+$'\n",
    "list_review_rev1 = []\n",
    "for row in list_review_rev:\n",
    "    list_tmp = []\n",
    "    for sentence in row:\n",
    "        list_tmp.append(re.sub(pattern, '', sentence))\n",
    "    list_review_rev1.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sentenceに変更、その際、ユーザー番号を頭に振る\n",
    "list_sentence = []\n",
    "for row in list_review_rev1:\n",
    "    list_tmp = []\n",
    "    for i, review in enumerate(row):\n",
    "        list_s = re.split(r'。|！|？', review)\n",
    "        for sen in list_s:\n",
    "            list_tmp.append([i, sen])\n",
    "    list_sentence.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 文字の前後の空白を削除\n",
    "list_sentence_rev = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        sentence = sen[1].replace(' ', '')\n",
    "        if len(sentence) > 1:\n",
    "            list_tmp.append([sen[0], sentence])\n",
    "    list_sentence_rev.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 先頭が、」もしくは）で始まってる場合、前の文にくっつける\n",
    "list_sentence_rev1 = []\n",
    "for row in list_sentence_rev:\n",
    "    list_tmp = []\n",
    "    for i in range(len(row)-1):\n",
    "        if re.match(r'」|』|\\)|）', row[i][1]):\n",
    "            pass\n",
    "        elif re.match(r'」|』|\\)|）', row[i+1][1]):\n",
    "            list_tmp.append([row[i][0], row[i][1]+row[i+1][1]])\n",
    "        else:\n",
    "            list_tmp.append([row[i][0], row[i][1]])\n",
    "    else:\n",
    "        list_tmp.append([row[i+1][0], row[i+1][1]])\n",
    "        list_sentence_rev1.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Filer.writetsv(list_sentence_rev1[0], '../files/sentencefile/sentence-51637.tsv')\n",
    "Filer.writetsv(list_sentence_rev1[1], '../files/sentencefile/sentence-1238.tsv')\n",
    "Filer.writetsv(list_sentence_rev1[2], '../files/sentencefile/sentence-53171.tsv')\n",
    "Filer.writetsv(list_sentence_rev1[3], '../files/sentencefile/sentence-19191.tsv')\n",
    "Filer.writetsv(list_sentence_rev1[4], '../files/sentencefile/sentence-76887.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### sentenceごとのファイルを形態素解析済みのファイルに変更する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../files/sentencefile/sentence-76887.tsv', '../files/sentencefile/sentence-1238.tsv', '../files/sentencefile/sentence-53171.tsv', '../files/sentencefile/sentence-19191.tsv', '../files/sentencefile/sentence-51637.tsv']\n"
     ]
    }
   ],
   "source": [
    "list_path = glob.glob('../files/sentencefile/*')\n",
    "print list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentence = [Filer.readtsv(path) for path in list_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_morpho = [[[sen[1], morpho(sen[1])] for sen in row]for row in list_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writedump(list_morpho[4], '../files/morphofile/type1/morpho-51637.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_morpho1 = [[[sen[1], morpho1(sen[1])] for sen in row] for row in list_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writedump(list_morpho1[4], '../files/morphofile/type2/morpho-51637.dump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高頻度語、低頻度語を数え上げて削除したファイルを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_word = [word for row in list_morpho for sen in row for word in sen[1]]\n",
    "list_word1 = [word for row in list_morpho1 for sen in row for word in sen[1]]\n",
    "dict_word_freq = collections.Counter(list_word)\n",
    "dict_word_freq1 = collections.Counter(list_word1)\n",
    "list_set_word = []\n",
    "for row in list_morpho:\n",
    "    for sen in row:\n",
    "        list_set_word.extend(list(set(sen[1])))\n",
    "        \n",
    "list_set_word1 = []\n",
    "for row in list_morpho1:\n",
    "    for sen in row:\n",
    "        list_set_word1.extend(list(set(sen[1])))\n",
    "\n",
    "dict_document_freq = collections.Counter(list_set_word)\n",
    "dict_document_freq1 = collections.Counter(list_set_word1)\n",
    "print len(list_morpho[0])+len(list_morpho[1])+len(list_morpho[2])+len(list_morpho[3])+len(list_morpho[4])\n",
    "print len(list_morpho1[0])+len(list_morpho1[1])+len(list_morpho1[2])+len(list_morpho1[3])+len(list_morpho1[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "部屋 0.166852010265\n",
      "利用 0.136783575706\n",
      "ホテル 0.100427715997\n",
      "良い 0.0860992301112\n",
      "宿泊 0.0706159110351\n",
      "満足 0.0563301967494\n",
      "便利 0.047005988024\n",
      "朝食 0.0458083832335\n",
      "よい 0.0455089820359\n",
      "広い 0.0444824636441\n"
     ]
    }
   ],
   "source": [
    "# 頻度の確認、10%切りで良さそう\n",
    "for row in sorted(dict_document_freq.items(), key=lambda x:x[1], reverse=True)[0:10]:\n",
    "    print row[0], row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 除去語の記録、10%以上、2回以下\n",
    "list_remove = []\n",
    "for row in dict_document_freq.items():\n",
    "    if row[1] >= 0.1:\n",
    "        list_remove.append(row[0])\n",
    "for row in dict_word_freq.items():\n",
    "    if row[1] <= 2:\n",
    "        list_remove.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Filer.writetxt(list_remove, '../files/preprocessedfile/dict/remove_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../files/morphofile/type1/morpho-51637.dump',\n",
       " '../files/morphofile/type1/morpho-53171.dump',\n",
       " '../files/morphofile/type1/morpho-1238.dump',\n",
       " '../files/morphofile/type1/morpho-76887.dump',\n",
       " '../files/morphofile/type1/morpho-19191.dump']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_path = glob.glob('../files/morphofile/type1/*.dump')\n",
    "list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentence = [Filer.readdump(path) for path in list_path]\n",
    "list_sentence_rev = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        list_tmp_tmp = [word for word in sen[1] if word not in list_remove]\n",
    "        list_tmp.append([sen[0], list_tmp_tmp])\n",
    "    list_sentence_rev.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 総単語数が１以下の文を削除\n",
    "list_sentence_rev = [[sen for sen in row if len(sen[1]) >= 2] for row in list_sentence_rev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writedump(list_sentence_rev[4], '../files/preprocessedfile/type11/wordlist/preprocessed-19191.dump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エッジリストの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../files/preprocessedfile/type11/wordlist/preprocessed-19191.dump',\n",
       " '../files/preprocessedfile/type11/wordlist/preprocessed-76887.dump',\n",
       " '../files/preprocessedfile/type11/wordlist/preprocessed-1238.dump',\n",
       " '../files/preprocessedfile/type11/wordlist/preprocessed-51637.dump',\n",
       " '../files/preprocessedfile/type11/wordlist/preprocessed-53171.dump']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_path = glob.glob('../files/preprocessedfile/type11/wordlist/*')\n",
    "list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentence = [Filer.readdump(path) for path in list_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bigram\n",
    "list_bigram = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        for i in range(len(sen[1])-1):\n",
    "            list_tmp.append([sen[1][i], sen[1][i+1]])\n",
    "    list_bigram.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writetsv(list_bigram[4], '../files/preprocessedfile/type11/bigram/bigram-53171.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cor\n",
    "list_cor = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        list_tmp.extend(list(itertools.combinations(tuple(sen[1]),2)))\n",
    "    list_tmp = [list(row) for row in list_tmp]\n",
    "    list_cor.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writetsv(list_cor[4], '../files/preprocessedfile/type11/cor/cor-53171.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'b'),\n",
       " ('a', 'c'),\n",
       " ('a', 'd'),\n",
       " ('a', 'e'),\n",
       " ('b', 'c'),\n",
       " ('b', 'd'),\n",
       " ('b', 'e'),\n",
       " ('c', 'd'),\n",
       " ('c', 'e'),\n",
       " ('d', 'e')]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = ['a', 'b', 'c', 'd', 'e']\n",
    "list(itertools.combinations(tuple(seq),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### PRTMによって、分類した後の文書を形態素解析する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_path = glob.glob('./files/classifiedfile/type11/bigram/sentence/*')\n",
    "removeword = './files/classifiedfile/type11/bigram/sentence/classified'\n",
    "list_name = [path.replace(removeword, '') for path in list_path]\n",
    "list_name = [name.replace('.txt', '') for name in list_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "list_sentence = [Filer.readtxt(path) for path in list_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 形態素解析\n",
    "list_master = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = [morpho_v(sentence) for sentence in row]\n",
    "    list_master.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "for row, name in zip(list_master, list_name):\n",
    "    Filer.writetsv(row, './files/classifiedfile/type11/bigram/morpho/n_adj_verb/classified'+name+'.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# エッジリストの作成\n",
    "list_bigram = []\n",
    "for row in list_master:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        for i in range(len(sen)-1):\n",
    "            list_tmp.append([sen[i], sen[i+1]])\n",
    "    list_bigram.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "for row, name in zip(list_bigram, list_name):\n",
    "    Filer.writetsv(row, './classifiedfile/type11/bigram/bigram/classified'+name+'.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cor\n",
    "list_cor = []\n",
    "for row in list_master:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        list_tmp.extend(list(itertools.combinations(tuple(sen),2)))\n",
    "    list_tmp = [list(row) for row in list_tmp]\n",
    "    list_cor.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "for row, name in zip(list_cor, list_name):\n",
    "    Filer.writetsv(row, './classifiedfile/type11/bigram/cor/classified'+name+'.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 構文構造を利用してエッジリストを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_path = glob.glob('files/classifiedfile/type11/bigram/sentence/classified*')\n",
    "removeword = 'files/classifiedfile/type11/bigram/sentence/classified'\n",
    "list_name = [path.replace(removeword, '') for path in list_path]\n",
    "list_name = [name.replace('.txt', '') for name in list_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "list_sentence = [Filer.readtxt(path) for path in list_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 構文によるエッジリストの作成(動詞なし)\n",
    "list_master = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = [[t[0], t[1]] for sentence in row for t in get_words(sentence)]\n",
    "    list_master.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開業 => 新横浜\n",
      "開業 => プリンス\n",
      "駅 => 近い\n",
      "近い => 近い\n",
      "アリーナ => 近い\n",
      "近い => 利用\n",
      "新横浜 => イベント\n",
      "横浜アリーナ => ライヴ\n",
      "ライヴ => あり\n",
      "ライヴ => 利用\n"
     ]
    }
   ],
   "source": [
    "for row in list_master[0][0:10]:\n",
    "    print row[0], '=>', row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "for row, name in zip(list_master, list_name):\n",
    "    Filer.writetsv(row, './files/classifiedfile/type11/bigram/syntax/non_verb/classified'+name+'.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 構文によるエッジリストの作成(動詞あり)\n",
    "list_master = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = [[t[0], t[1]] for sentence in row for t in get_v_words(sentence)]\n",
    "    list_master.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "for row, name in zip(list_master, list_name):\n",
    "    Filer.writetsv(row, './files/classifiedfile/type11/bigram/syntax/verb/classified'+name+'.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "フロント  =>  配置\n",
      "フロント  =>  欲しい\n",
      "人  =>  配置\n",
      "人  =>  欲しい\n"
     ]
    }
   ],
   "source": [
    "sentence = '時間的に混むのがわかったいるんだから、もう少しフロントに人を配置して欲しい'\n",
    "t = get_words(sentence)\n",
    "for row in t:\n",
    "    print row[0], ' => ', row[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### OpinosisDatasetの基礎集計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "list_path_corpus = glob.glob('./files/OpinosisDataset1.0/topics/*')\n",
    "list_path_gold = glob.glob('./files/OpinosisDataset1.0/summaries-gold/*')\n",
    "print len(list_path_corpus)\n",
    "print len(list_path_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Garmin seems to be generally very accurate.',\n",
       " \"It's easy to use with an intuitive interface.\"]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_corpus_gold = Filer.readtxt('./files/OpinosisDataset1.0/summaries-gold/accuracy_garmin_nuvi_255W_gps/accuracy_garmin_nuvi_255W_gps.2.gold', LF='\\r\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpinosisDatasetの形態素解析\n",
    "1. トピックごとにdictを作成する\n",
    " * sentence: 元の文のリスト\n",
    " * sep_all: 含まれているすべての単語のリスト\n",
    " * sep_nj: 含まれている名詞と形容詞のリスト\n",
    " * sep_njv: 含まれている名詞と形容詞と動詞のリスト\n",
    " * edge_nj: すべての単語の構文によるエッジ\n",
    " * edge_nj: 名詞と形容詞の構文によるエッジ\n",
    " * edge_njv: 名詞と形容詞と動詞の構文によるエッジ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_path = glob.glob('./files/OpinosisDataset1.0/topics/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CoreNLP_PyWrapper:Starting java subprocess, and waiting for signal it's ready, with command: exec java -Xmx4g -XX:ParallelGCThreads=1 -cp '/home/ikegami/ikegami/lib/python2.7/site-packages/stanford_corenlp_pywrapper/lib/*:/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*'      corenlp.SocketServer --outpipe /tmp/corenlp_pywrap_pipe_pypid=12569_time=1472178902.01  --configdict '{\"annotators\": \"tokenize, ssplit, pos, lemma, parse\"}'\n",
      "INFO:CoreNLP_PyWrapper:Successful ping. The server has started.\n",
      "INFO:CoreNLP_PyWrapper:Subprocess is ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./files/OpinosisDataset1.0/preprocessed1/gas_mileage_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/location_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/interior_honda_accord_2008.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/sound_ipod_nano_8gb.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/size_asus_netbook_1005ha.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/eyesight-issues_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/display_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/video_ipod_nano_8gb.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/buttons_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/screen_ipod_nano_8gb.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/location_holiday_inn_london.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/comfort_honda_accord_2008.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/service_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/features_windows7.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/fonts_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/bathroom_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/mileage_honda_accord_2008.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/screen_netbook_1005ha.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/speed_windows7.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/interior_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/staff_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/seats_honda_accord_2008.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/food_swissotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/quality_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/parking_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/voice_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/battery-life_ipod_nano_8gb.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/service_swissotel_hotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/screen_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/directions_garmin_nuvi_255W_gps.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/free_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/speed_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/navigation_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/satellite_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/performance_netbook_1005ha.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/price_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/transmission_toyota_camry_2007.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/comfort_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/food_holiday_inn_london.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/updates_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/accuracy_garmin_nuvi_255W_gps.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/price_holiday_inn_london.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/service_holiday_inn_london.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/battery-life_netbook_1005ha.dump\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/rooms_swissotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/keyboard_netbook_1005ha.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/battery-life_amazon_kindle.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/room_holiday_inn_london.dump\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/rooms_bestwestern_hotel_sfo.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/staff_swissotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/performance_honda_accord_2008.dump\n"
     ]
    }
   ],
   "source": [
    "proc = CoreNLP(configdict={'annotators': 'tokenize, ssplit, pos, lemma, parse'},\n",
    "               corenlp_jars=[\"/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*\"])\n",
    "\n",
    "for path in list_path:\n",
    "    dict_total = {'sentence':[],\n",
    "                  'sep_all':[],\n",
    "                  'sep_nj':[],\n",
    "                  'sep_njv':[],\n",
    "                  'edge_all': [],\n",
    "                  'edge_nj':[],\n",
    "                  'edge_njv': []}\n",
    "    filename = path.replace('./files/OpinosisDataset1.0/topics/', '').replace('.txt.data', '')\n",
    "    outputpath = './files/OpinosisDataset1.0/preprocessed1/'+filename+'.dump'\n",
    "    list_corpus = Filer.readtxt(path, LF='\\r\\n')\n",
    "    for corpus in list_corpus:\n",
    "        try:\n",
    "            list_sentence_a, list_bag_a, list_edge_a = morpho_e(corpus)\n",
    "            list_sentence_e, list_bag_e, list_edge_e = morpho_e_nj(corpus)\n",
    "            list_sentence_v, list_bag_v, list_edge_v = morpho_e_njv(corpus)\n",
    "            dict_total['sentence'].extend(list_sentence_e)\n",
    "            dict_total['sep_all'].extend(list_bag_a)\n",
    "            dict_total['sep_nj'].extend(list_bag_e)\n",
    "            dict_total['sep_njv'].extend(list_bag_v)\n",
    "            dict_total['edge_all'].extend(list_edge_a)\n",
    "            dict_total['edge_nj'].extend(list_edge_e)\n",
    "            dict_total['edge_njv'].extend(list_edge_v)\n",
    "        except UnicodeDecodeError:\n",
    "            print 'error'\n",
    "    Filer.writedump(dict_total, outputpath)\n",
    "    print outputpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpinosisDataset.goldの形態素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_path = glob.glob('./files/OpinosisDataset1.0/summaries-gold/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./files/OpinosisDataset1.0/summaries-gold/bathroom_bestwestern_hotel_sfo',\n",
       " './files/OpinosisDataset1.0/summaries-gold/staff_swissotel_chicago',\n",
       " './files/OpinosisDataset1.0/summaries-gold/navigation_amazon_kindle',\n",
       " './files/OpinosisDataset1.0/summaries-gold/performance_netbook_1005ha',\n",
       " './files/OpinosisDataset1.0/summaries-gold/free_bestwestern_hotel_sfo',\n",
       " './files/OpinosisDataset1.0/summaries-gold/speed_windows7',\n",
       " './files/OpinosisDataset1.0/summaries-gold/eyesight-issues_amazon_kindle',\n",
       " './files/OpinosisDataset1.0/summaries-gold/staff_bestwestern_hotel_sfo',\n",
       " './files/OpinosisDataset1.0/summaries-gold/sound_ipod_nano_8gb',\n",
       " './files/OpinosisDataset1.0/summaries-gold/screen_netbook_1005ha',\n",
       " './files/OpinosisDataset1.0/summaries-gold/features_windows7',\n",
       " './files/OpinosisDataset1.0/summaries-gold/fonts_amazon_kindle',\n",
       " './files/OpinosisDataset1.0/summaries-gold/interior_toyota_camry_2007',\n",
       " './files/OpinosisDataset1.0/summaries-gold/performance_honda_accord_2008',\n",
       " './files/OpinosisDataset1.0/summaries-gold/service_bestwestern_hotel_sfo',\n",
       " './files/OpinosisDataset1.0/summaries-gold/location_holiday_inn_london',\n",
       " './files/OpinosisDataset1.0/summaries-gold/service_swissotel_hotel_chicago',\n",
       " './files/OpinosisDataset1.0/summaries-gold/directions_garmin_nuvi_255W_gps',\n",
       " './files/OpinosisDataset1.0/summaries-gold/buttons_amazon_kindle',\n",
       " './files/OpinosisDataset1.0/summaries-gold/price_holiday_inn_london',\n",
       " './files/OpinosisDataset1.0/summaries-gold/food_holiday_inn_london',\n",
       " './files/OpinosisDataset1.0/summaries-gold/battery-life_amazon_kindle',\n",
       " './files/OpinosisDataset1.0/summaries-gold/location_bestwestern_hotel_sfo',\n",
       " './files/OpinosisDataset1.0/summaries-gold/updates_garmin_nuvi_255W_gps',\n",
       " './files/OpinosisDataset1.0/summaries-gold/accuracy_garmin_nuvi_255W_gps',\n",
       " './files/OpinosisDataset1.0/summaries-gold/battery-life_ipod_nano_8gb',\n",
       " './files/OpinosisDataset1.0/summaries-gold/display_garmin_nuvi_255W_gps',\n",
       " './files/OpinosisDataset1.0/summaries-gold/food_swissotel_chicago',\n",
       " './files/OpinosisDataset1.0/summaries-gold/voice_garmin_nuvi_255W_gps',\n",
       " './files/OpinosisDataset1.0/summaries-gold/transmission_toyota_camry_2007',\n",
       " './files/OpinosisDataset1.0/summaries-gold/rooms_bestwestern_hotel_sfo',\n",
       " './files/OpinosisDataset1.0/summaries-gold/keyboard_netbook_1005ha',\n",
       " './files/OpinosisDataset1.0/summaries-gold/seats_honda_accord_2008',\n",
       " './files/OpinosisDataset1.0/summaries-gold/gas_mileage_toyota_camry_2007',\n",
       " './files/OpinosisDataset1.0/summaries-gold/parking_bestwestern_hotel_sfo',\n",
       " './files/OpinosisDataset1.0/summaries-gold/screen_ipod_nano_8gb',\n",
       " './files/OpinosisDataset1.0/summaries-gold/interior_honda_accord_2008',\n",
       " './files/OpinosisDataset1.0/summaries-gold/size_asus_netbook_1005ha',\n",
       " './files/OpinosisDataset1.0/summaries-gold/video_ipod_nano_8gb',\n",
       " './files/OpinosisDataset1.0/summaries-gold/mileage_honda_accord_2008',\n",
       " './files/OpinosisDataset1.0/summaries-gold/screen_garmin_nuvi_255W_gps',\n",
       " './files/OpinosisDataset1.0/summaries-gold/comfort_honda_accord_2008',\n",
       " './files/OpinosisDataset1.0/summaries-gold/price_amazon_kindle',\n",
       " './files/OpinosisDataset1.0/summaries-gold/battery-life_netbook_1005ha',\n",
       " './files/OpinosisDataset1.0/summaries-gold/quality_toyota_camry_2007',\n",
       " './files/OpinosisDataset1.0/summaries-gold/service_holiday_inn_london',\n",
       " './files/OpinosisDataset1.0/summaries-gold/satellite_garmin_nuvi_255W_gps',\n",
       " './files/OpinosisDataset1.0/summaries-gold/rooms_swissotel_chicago',\n",
       " './files/OpinosisDataset1.0/summaries-gold/speed_garmin_nuvi_255W_gps',\n",
       " './files/OpinosisDataset1.0/summaries-gold/comfort_toyota_camry_2007',\n",
       " './files/OpinosisDataset1.0/summaries-gold/room_holiday_inn_london']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CoreNLP_PyWrapper:Starting java subprocess, and waiting for signal it's ready, with command: exec java -Xmx4g -XX:ParallelGCThreads=1 -cp '/home/ikegami/ikegami/lib/python2.7/site-packages/stanford_corenlp_pywrapper/lib/*:/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*'      corenlp.SocketServer --outpipe /tmp/corenlp_pywrap_pipe_pypid=12569_time=1472184282.29  --configdict '{\"annotators\": \"tokenize, ssplit, pos, lemma, parse\"}'\n",
      "INFO:CoreNLP_PyWrapper:Successful ping. The server has started.\n",
      "INFO:CoreNLP_PyWrapper:Subprocess is ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathroom_bestwestern_hotel_sfo\n",
      "staff_swissotel_chicago\n",
      "navigation_amazon_kindle\n",
      "performance_netbook_1005ha\n",
      "free_bestwestern_hotel_sfo\n",
      "speed_windows7\n",
      "eyesight-issues_amazon_kindle\n",
      "staff_bestwestern_hotel_sfo\n",
      "sound_ipod_nano_8gb\n",
      "screen_netbook_1005ha\n",
      "features_windows7\n",
      "fonts_amazon_kindle\n",
      "interior_toyota_camry_2007\n",
      "performance_honda_accord_2008\n",
      "service_bestwestern_hotel_sfo\n",
      "location_holiday_inn_london\n",
      "service_swissotel_hotel_chicago\n",
      "directions_garmin_nuvi_255W_gps\n",
      "buttons_amazon_kindle\n",
      "price_holiday_inn_london\n",
      "food_holiday_inn_london\n",
      "battery-life_amazon_kindle\n",
      "location_bestwestern_hotel_sfo\n",
      "updates_garmin_nuvi_255W_gps\n",
      "accuracy_garmin_nuvi_255W_gps\n",
      "battery-life_ipod_nano_8gb\n",
      "display_garmin_nuvi_255W_gps\n",
      "food_swissotel_chicago\n",
      "voice_garmin_nuvi_255W_gps\n",
      "transmission_toyota_camry_2007\n",
      "rooms_bestwestern_hotel_sfo\n",
      "keyboard_netbook_1005ha\n",
      "seats_honda_accord_2008\n",
      "gas_mileage_toyota_camry_2007\n",
      "parking_bestwestern_hotel_sfo\n",
      "screen_ipod_nano_8gb\n",
      "interior_honda_accord_2008\n",
      "size_asus_netbook_1005ha\n",
      "video_ipod_nano_8gb\n",
      "mileage_honda_accord_2008\n",
      "screen_garmin_nuvi_255W_gps\n",
      "comfort_honda_accord_2008\n",
      "price_amazon_kindle\n",
      "battery-life_netbook_1005ha\n",
      "quality_toyota_camry_2007\n",
      "service_holiday_inn_london\n",
      "satellite_garmin_nuvi_255W_gps\n",
      "rooms_swissotel_chicago\n",
      "speed_garmin_nuvi_255W_gps\n",
      "comfort_toyota_camry_2007\n",
      "room_holiday_inn_london\n"
     ]
    }
   ],
   "source": [
    "proc = CoreNLP(configdict={'annotators': 'tokenize, ssplit, pos, lemma, parse'},\n",
    "               corenlp_jars=[\"/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*\"])\n",
    "\n",
    "for path in list_path:\n",
    "    filename = path.replace('./files/OpinosisDataset1.0/summaries-gold/', '')\n",
    "    list_filepath = glob.glob(path+'/*')\n",
    "    dict_total = {'sentence':[],\n",
    "                  'sep_all':[]}\n",
    "    for path1 in list_filepath:\n",
    "        list_sentence = []\n",
    "        list_bag = []\n",
    "        list_text = Filer.readtxt(path1, LF='\\r\\n')\n",
    "        for sentence in list_text:\n",
    "            list_sentence_a, list_bag_a, _ = morpho_e(sentence)\n",
    "            list_sentence.extend(list_sentence_a)\n",
    "            list_bag.extend(list_bag_a)\n",
    "        dict_total['sentence'].append(list_sentence)\n",
    "        dict_total['sep_all'].append(list_bag)\n",
    "    print filename\n",
    "    Filer.writedump(dict_total, './files/OpinosisDataset1.0/preprocessed1/ans/'+filename+'.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_file = Filer.readdump('./files/OpinosisDataset1.0/preprocessed1/test/accuracy_garmin_nuvi_255W_gps.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ./files/OpinosisDataset1.0/preprocessed/test/eyesight-issues_amazon_kindle.dump\n",
      "all 437 433 True\n",
      "nj 234 197 True\n",
      "njv 315 314 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/food_swissotel_chicago.dump\n",
      "all 391 391 True\n",
      "nj 244 226 True\n",
      "njv 296 296 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/speed_windows7.dump\n",
      "all 546 542 True\n",
      "nj 313 274 True\n",
      "njv 396 391 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/features_windows7.dump\n",
      "all 414 411 True\n",
      "nj 227 198 True\n",
      "njv 305 302 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/display_garmin_nuvi_255W_gps.dump\n",
      "all 347 345 True\n",
      "nj 195 167 True\n",
      "njv 251 246 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/interior_toyota_camry_2007.dump\n",
      "all 527 524 True\n",
      "nj 334 301 True\n",
      "njv 410 405 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/video_ipod_nano_8gb.dump\n",
      "all 680 675 True\n",
      "nj 440 385 True\n",
      "njv 541 537 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/rooms_bestwestern_hotel_sfo.dump\n",
      "all 890 886 True\n",
      "nj 545 490 True\n",
      "njv 687 683 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/interior_honda_accord_2008.dump\n",
      "all 495 492 True\n",
      "nj 317 293 True\n",
      "njv 381 378 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/service_holiday_inn_london.dump\n",
      "all 764 761 True\n",
      "nj 487 435 True\n",
      "njv 592 588 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/staff_swissotel_chicago.dump\n",
      "all 705 702 True\n",
      "nj 442 400 True\n",
      "njv 552 544 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/screen_netbook_1005ha.dump\n",
      "all 834 833 True\n",
      "nj 516 466 True\n",
      "njv 657 653 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/location_holiday_inn_london.dump\n",
      "all 908 905 True\n",
      "nj 614 573 True\n",
      "njv 728 721 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/price_amazon_kindle.dump\n",
      "all 527 523 True\n",
      "nj 276 232 True\n",
      "njv 372 370 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/gas_mileage_toyota_camry_2007.dump\n",
      "all 493 491 True\n",
      "nj 278 247 True\n",
      "njv 346 343 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/price_holiday_inn_london.dump\n",
      "all 545 543 True\n",
      "nj 334 300 True\n",
      "njv 415 414 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/size_asus_netbook_1005ha.dump\n",
      "all 539 538 True\n",
      "nj 323 294 True\n",
      "njv 404 401 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/speed_garmin_nuvi_255W_gps.dump\n",
      "all 406 404 True\n",
      "nj 222 194 True\n",
      "njv 287 284 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/comfort_honda_accord_2008.dump\n",
      "all 617 614 True\n",
      "nj 385 352 True\n",
      "njv 472 469 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/voice_garmin_nuvi_255W_gps.dump\n",
      "all 451 450 True\n",
      "nj 262 220 True\n",
      "njv 337 335 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/staff_bestwestern_hotel_sfo.dump\n",
      "all 893 891 True\n",
      "nj 580 519 True\n",
      "njv 716 712 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/screen_garmin_nuvi_255W_gps.dump\n",
      "all 529 525 True\n",
      "nj 296 255 True\n",
      "njv 396 395 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/service_swissotel_hotel_chicago.dump\n",
      "all 857 853 True\n",
      "nj 534 471 True\n",
      "njv 671 664 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/battery-life_amazon_kindle.dump\n",
      "all 476 475 True\n",
      "nj 250 209 True\n",
      "njv 336 331 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/performance_netbook_1005ha.dump\n",
      "all 383 382 True\n",
      "nj 213 192 True\n",
      "njv 280 280 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/parking_bestwestern_hotel_sfo.dump\n",
      "all 454 451 True\n",
      "nj 245 207 True\n",
      "njv 320 316 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/performance_honda_accord_2008.dump\n",
      "all 309 308 True\n",
      "nj 173 160 True\n",
      "njv 216 213 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/battery-life_netbook_1005ha.dump\n",
      "all 1132 1128 True\n",
      "nj 716 643 True\n",
      "njv 907 901 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/location_bestwestern_hotel_sfo.dump\n",
      "all 788 785 True\n",
      "nj 540 504 True\n",
      "njv 631 626 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/keyboard_netbook_1005ha.dump\n",
      "all 518 517 True\n",
      "nj 311 279 True\n",
      "njv 385 384 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/service_bestwestern_hotel_sfo.dump\n",
      "all 646 643 True\n",
      "nj 403 366 True\n",
      "njv 505 502 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/buttons_amazon_kindle.dump\n",
      "all 686 682 True\n",
      "nj 363 317 True\n",
      "njv 512 507 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/satellite_garmin_nuvi_255W_gps.dump\n",
      "all 381 378 True\n",
      "nj 187 154 True\n",
      "njv 257 255 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/directions_garmin_nuvi_255W_gps.dump\n",
      "all 579 576 True\n",
      "nj 321 275 True\n",
      "njv 445 441 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/sound_ipod_nano_8gb.dump\n",
      "all 409 405 True\n",
      "nj 253 230 True\n",
      "njv 318 315 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/room_holiday_inn_london.dump\n",
      "all 1597 1593 True\n",
      "nj 1065 963 True\n",
      "njv 1316 1307 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/rooms_swissotel_chicago.dump\n",
      "all 668 664 True\n",
      "nj 402 362 True\n",
      "njv 500 492 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/quality_toyota_camry_2007.dump\n",
      "all 346 345 True\n",
      "nj 184 165 True\n",
      "njv 243 240 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/updates_garmin_nuvi_255W_gps.dump\n",
      "all 416 415 True\n",
      "nj 220 186 True\n",
      "njv 291 288 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/bathroom_bestwestern_hotel_sfo.dump\n",
      "all 399 397 True\n",
      "nj 223 195 True\n",
      "njv 288 286 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/seats_honda_accord_2008.dump\n",
      "all 422 417 True\n",
      "nj 253 226 True\n",
      "njv 309 306 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/mileage_honda_accord_2008.dump\n",
      "all 575 571 True\n",
      "nj 317 279 True\n",
      "njv 405 400 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/transmission_toyota_camry_2007.dump\n",
      "all 648 645 True\n",
      "nj 357 305 True\n",
      "njv 468 464 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/free_bestwestern_hotel_sfo.dump\n",
      "all 509 504 True\n",
      "nj 301 263 True\n",
      "njv 382 377 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/comfort_toyota_camry_2007.dump\n",
      "all 496 493 True\n",
      "nj 292 261 True\n",
      "njv 364 358 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/accuracy_garmin_nuvi_255W_gps.dump\n",
      "all 364 363 True\n",
      "nj 189 160 True\n",
      "njv 254 253 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/navigation_amazon_kindle.dump\n",
      "all 417 415 True\n",
      "nj 232 212 True\n",
      "njv 293 291 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/food_holiday_inn_london.dump\n",
      "all 547 543 True\n",
      "nj 358 331 True\n",
      "njv 428 426 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/battery-life_ipod_nano_8gb.dump\n",
      "all 339 338 True\n",
      "nj 194 169 True\n",
      "njv 249 249 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/screen_ipod_nano_8gb.dump\n",
      "all 354 353 True\n",
      "nj 220 204 True\n",
      "njv 264 263 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/fonts_amazon_kindle.dump\n",
      "all 423 420 True\n",
      "nj 234 205 True\n",
      "njv 308 307 True\n"
     ]
    }
   ],
   "source": [
    "filepath = glob.glob('./files/OpinosisDataset1.0/preprocessed/test/*')\n",
    "for path in filepath:\n",
    "    print path\n",
    "    dict_tmp = Filer.readdump(path)\n",
    "    list_word = set([word for row in dict_tmp['sep_all'] for word in row])\n",
    "    list_edge = set([word for row in dict_tmp['edge_all'] for words in row for word in words])\n",
    "    print 'all', len(list_word), len(list_edge), list_edge.issubset(list_word)\n",
    "    list_word = set([word for row in dict_tmp['sep_nj'] for word in row])\n",
    "    list_edge = set([word for row in dict_tmp['edge_nj'] for words in row for word in words])\n",
    "    print 'nj', len(list_word), len(list_edge), list_edge.issubset(list_word)\n",
    "    list_word = set([word for row in dict_tmp['sep_njv'] for word in row])\n",
    "    list_edge = set([word for row in dict_tmp['edge_njv'] for words in row for word in words])\n",
    "    print 'njv', len(list_word), len(list_edge), list_edge.issubset(list_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_tmp = Filer.readdump('./files/OpinosisDataset1.0/preprocessed/test/accuracy_garmin_nuvi_255W_gps.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./files/OpinosisDataset1.0/preprocessed/test/sound_ipod_nano_8gb.dump\n"
     ]
    }
   ],
   "source": [
    "filepath = glob.glob('./files/OpinosisDataset1.0/preprocessed/test/*')\n",
    "for path in filepath:\n",
    "    dict_tmp = Filer.readdump(path)\n",
    "    if len(dict_tmp['sentence']) == 101:\n",
    "        print path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### OpinosisDatasetの形態素解析\n",
    "1. トピックごとにdictを作成する\n",
    " * sentence: 元の文のリスト\n",
    " * sep_all: 含まれているすべての単語のリスト\n",
    " * sep_nj: 含まれている名詞と形容詞のリスト\n",
    " * sep_njv: 含まれている名詞と形容詞と動詞のリスト\n",
    " * pos_all: 含まれているすべての単語のpos\n",
    " * pos_nj: 含まれている名詞と形容詞のpos\n",
    " * pos_njv: 含まれている名詞と形容詞と動詞のpos\n",
    " * edge_nj: すべての単語の構文によるエッジ\n",
    " * edge_nj: 名詞と形容詞の構文によるエッジ\n",
    " * edge_njv: 名詞と形容詞と動詞の構文によるエッジ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_path = glob.glob('./files/OpinosisDataset1.0/topics/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CoreNLP_PyWrapper:Starting java subprocess, and waiting for signal it's ready, with command: exec java -Xmx4g -XX:ParallelGCThreads=1 -cp '/home/ikegami/ikegami/lib/python2.7/site-packages/stanford_corenlp_pywrapper/lib/*:/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*'      corenlp.SocketServer --outpipe /tmp/corenlp_pywrap_pipe_pypid=11416_time=1472539223.36  --configdict '{\"annotators\": \"tokenize, ssplit, pos, lemma, parse\"}'\n",
      "INFO:CoreNLP_PyWrapper:Successful ping. The server has started.\n",
      "INFO:CoreNLP_PyWrapper:Subprocess is ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./files/OpinosisDataset1.0/preprocessed3/gas_mileage_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/location_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/interior_honda_accord_2008.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/sound_ipod_nano_8gb.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/size_asus_netbook_1005ha.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/eyesight-issues_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/display_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/video_ipod_nano_8gb.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/buttons_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/screen_ipod_nano_8gb.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/location_holiday_inn_london.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/comfort_honda_accord_2008.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/service_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/features_windows7.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/fonts_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/bathroom_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/mileage_honda_accord_2008.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/screen_netbook_1005ha.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/speed_windows7.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/interior_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/staff_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/seats_honda_accord_2008.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/food_swissotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/quality_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/parking_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/voice_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/battery-life_ipod_nano_8gb.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/service_swissotel_hotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/screen_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/directions_garmin_nuvi_255W_gps.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/free_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/speed_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/navigation_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/satellite_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/performance_netbook_1005ha.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/price_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/transmission_toyota_camry_2007.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/comfort_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/food_holiday_inn_london.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/updates_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/accuracy_garmin_nuvi_255W_gps.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/price_holiday_inn_london.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/service_holiday_inn_london.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/battery-life_netbook_1005ha.dump\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/rooms_swissotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/keyboard_netbook_1005ha.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/battery-life_amazon_kindle.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/room_holiday_inn_london.dump\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/rooms_bestwestern_hotel_sfo.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed3/staff_swissotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed3/performance_honda_accord_2008.dump\n"
     ]
    }
   ],
   "source": [
    "proc = CoreNLP(configdict={'annotators': 'tokenize, ssplit, pos, lemma, parse'},\n",
    "               corenlp_jars=[\"/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*\"])\n",
    "\n",
    "for path in list_path:\n",
    "    dict_total = {'sentence':[],\n",
    "                  'sep_all':[],\n",
    "                  'sep_nj':[],\n",
    "                  'sep_njv':[],\n",
    "                  'pos_all':[],\n",
    "                  'pos_nj':[],\n",
    "                  'pos_njv':[],\n",
    "                  'edge_all': [],\n",
    "                  'edge_nj':[],\n",
    "                  'edge_njv': []}\n",
    "    filename = path.replace('./files/OpinosisDataset1.0/topics/', '').replace('.txt.data', '')\n",
    "    outputpath = './files/OpinosisDataset1.0/preprocessed3/'+filename+'.dump'\n",
    "    list_corpus = Filer.readtxt(path, LF='\\r\\n')\n",
    "    for corpus in list_corpus:\n",
    "        try:\n",
    "            list_sentence_a, list_bag_a, list_edge_a, list_pos_a = morpho_e_pos(corpus)\n",
    "            list_sentence_e, list_bag_e, list_edge_e, list_pos_e = morpho_e_nj_pos(corpus)\n",
    "            list_sentence_v, list_bag_v, list_edge_v, list_pos_v = morpho_e_njv_pos(corpus)\n",
    "            dict_total['sentence'].extend(list_sentence_e)\n",
    "            dict_total['sep_all'].extend(list_bag_a)\n",
    "            dict_total['sep_nj'].extend(list_bag_e)\n",
    "            dict_total['sep_njv'].extend(list_bag_v)\n",
    "            dict_total['pos_all'].extend(list_pos_a)\n",
    "            dict_total['pos_nj'].extend(list_pos_e)\n",
    "            dict_total['pos_njv'].extend(list_pos_v)\n",
    "            dict_total['edge_all'].extend(list_edge_a)\n",
    "            dict_total['edge_nj'].extend(list_edge_e)\n",
    "            dict_total['edge_njv'].extend(list_edge_v)\n",
    "        except UnicodeDecodeError:\n",
    "            print 'error'\n",
    "    Filer.writedump(dict_total, outputpath)\n",
    "    print outputpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 楽天市場のデータの集計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.field_size_limit(1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201001_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201002_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201003_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201004_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201005_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201006_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201007_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201008_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201009_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201010_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201011_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201012_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201101_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201102_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201103_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201104_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201105_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201106_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201107_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201108_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201109_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201110_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201111_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201112_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201201_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201202_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201203_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201204_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201205_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201206_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201207_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201208_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201209_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201210_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201211_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201212_20140221.tsv\n"
     ]
    }
   ],
   "source": [
    "# 商品数の集計\n",
    "list_path = glob.glob('./files/rakuten_ichiba/rawfile/tsvfile/*')\n",
    "list_path.sort()\n",
    "list_product = []\n",
    "for path in list_path:\n",
    "    print path\n",
    "    list_tmp = Filer.readtsv(path)\n",
    "    list_tmp = [row[3] for row in list_tmp]\n",
    "    list_product.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darkangel:312-535-01216 28257\n",
      "donya:88584-ss 21796\n",
      "soukai:9000009984074 16563\n",
      "angfa:648226 14003\n",
      "z-craft:1239 11524\n",
      "low-ya:vg-groony 11443\n",
      "tea-life:90199 11167\n",
      "gourmetcoffee:2645 10228\n",
      "shaddy:116961130set 10210\n",
      "aimere:e0020014 9607\n",
      "pierrot:a1202-012414 9373\n",
      "kyunan:1418025 9349\n",
      "morrymama:10000134 8946\n",
      "hseason:865804 8872\n",
      "oga:1011967 8785\n",
      "oga:1437888 8456\n",
      "kyunan:mirainokouso-premium 8059\n",
      "aarti:10000089 7909\n",
      "hobinavi:time4 7878\n",
      "kumamoto-food:10000601 7710\n"
     ]
    }
   ],
   "source": [
    "# 2012年の商品ごとのレコード数 => 2012年だけ考えれば十分\n",
    "list_2012 = []\n",
    "for row in list_product[24:]:\n",
    "    list_2012.extend(row)\n",
    "\n",
    "dict_count = collections.Counter(list_2012)\n",
    "for row in sorted(dict_count.items(), key=lambda x:x[1], reverse=True)[0:20]:\n",
    "    print row[0], row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pierrot:a1202-012414 9373\n",
      "kyunan:1418025 9349\n",
      "morrymama:10000134 8946\n",
      "hseason:865804 8872\n",
      "oga:1011967 8785\n",
      "oga:1437888 8456\n",
      "kyunan:mirainokouso-premium 8059\n",
      "aarti:10000089 7909\n",
      "hobinavi:time4 7878\n",
      "kumamoto-food:10000601 7710\n",
      "sawaicoffee-tea:saledripbag 7668\n",
      "maxshare:a05750_sale 7626\n",
      "nat-gar:diet_fastyplacenta 7529\n",
      "lala-boutique:7380043 7330\n",
      "z-craft:1239-0076 7222\n",
      "arai:555329 7121\n",
      "ecojiji:enelong_3_1 7065\n",
      "seasky:ink-100en 6959\n",
      "kira-kira:kmb0001 6949\n",
      "low-ya:vg-pola 6893\n",
      "darkangel:nctop-6670 6885\n",
      "childhoodpepit:10000984 6843\n",
      "netstar:leg002 6814\n",
      "kyunan:20grain 6728\n",
      "iloveheaven:wkic01 6689\n",
      "bestmart:103368 6660\n",
      "vour-voir:vb325 6623\n",
      "freshveg:03-076 6517\n",
      "minamiya-shop:945199 6413\n",
      "apm24:mjsc40201n-time-100 6400\n"
     ]
    }
   ],
   "source": [
    "# 多すぎても困るので、10位から40位までの30商品文のreviewを取得\n",
    "for row in sorted(dict_count.items(), key=lambda x:x[1], reverse=True)[10:40]:\n",
    "    print row[0], row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201206_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201202_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201208_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201203_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201205_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201210_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201204_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201211_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201207_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201209_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201201_20140221.tsv\n",
      "./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review201212_20140221.tsv\n"
     ]
    }
   ],
   "source": [
    "# 特定の商品を取得\n",
    "list_products = [\"pierrot:a1202-012414\", \"kyunan:1418025\", \"morrymama:10000134\",\n",
    "                 \"hseason:865804\", \"oga:1011967\", \"oga:1437888\",\n",
    "                 \"kyunan:mirainokouso-premium\", \"aarti:10000089\", \"hobinavi:time4\",\n",
    "                 \"kumamoto-food:10000601\", \"sawaicoffee-tea:saledripbag\", \"maxshare:a05750_sale\",\n",
    "                 \"nat-gar:diet_fastyplacenta\", \"lala-boutique:7380043\", \"z-craft:1239-0076\",\n",
    "                 \"arai:555329\", \"ecojiji:enelong_3_1\", \"seasky:ink-100en\",\n",
    "                 \"kira-kira:kmb0001\", \"low-ya:vg-pola\", \"darkangel:nctop-6670\",\n",
    "                 \"childhoodpepit:10000984\", \"netstar:leg002\", \"kyunan:20grain\",\n",
    "                 \"iloveheaven:wkic01\", \"bestmart:103368\", \"vour-voir:vb325\",\n",
    "                 \"freshveg:03-076\", \"minamiya-shop:945199\", \"apm24:mjsc40201n-time-100\"]\n",
    "\n",
    "dict_products = {product:[] for product in list_products}\n",
    "\n",
    "list_path = glob.glob('./files/rakuten_ichiba/rawfile/tsvfile/ichiba04_review2012*')\n",
    "for path in list_path:\n",
    "    print path\n",
    "    list_tmp = Filer.readtsv(path)\n",
    "    list_tmp = [row for row in list_tmp if row[3] in list_products]\n",
    "    for row in list_tmp:\n",
    "        dict_products[row[3]].append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 商品ごとにtsvに保存する\n",
    "for product in list_products:\n",
    "    Filer.writetsv(dict_products[product],\n",
    "                   './files/rakuten_ichiba/rawfile/productfile/'+product+'.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reviewの部分のみ抜き出す\n",
    "list_path = glob.glob('./files/rakuten_ichiba/rawfile/productfile/*')\n",
    "for path in list_path:\n",
    "    name = path.replace('./files/rakuten_ichiba/rawfile/productfile/', '')\n",
    "    list_tmp = Filer.readtsv(path)\n",
    "    list_tmp = [[row[15]] for row in list_tmp]\n",
    "    Filer.writetsv(list_tmp, './files/rakuten_ichiba/processedfile/reviewfile/'+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 文単位に分ける\n",
    "list_path = glob.glob('./files/rakuten_ichiba/processedfile/reviewfile/*')\n",
    "for path in list_path:\n",
    "    name = path.replace('./files/rakuten_ichiba/processedfile/reviewfile/', '')\n",
    "    list_tmp = Filer.readtsv(path)\n",
    "    list_sentence = []\n",
    "    # ビックリマークを句点に変える\n",
    "    for sen in list_tmp:\n",
    "        sentence = sen[0].replace('!', '。').replace('！', '。')\n",
    "        list_sentence.extend(sentence.split('。'))\n",
    "    list_sentence = [[row] for row in list_sentence if row != '']\n",
    "    Filer.writetsv(list_sentence, './files/rakuten_ichiba/processedfile/sentencefile/'+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 文を形態素解析して、list_dict_sentence_allword_wordを作る\n",
    "list_path = glob.glob('./files/rakuten_ichiba/processedfile/sentencefile/*')\n",
    "for num, path in enumerate(list_path):\n",
    "    list_tmp = Filer.readtsv(path)\n",
    "    list_master = []\n",
    "    for sen in list_tmp:\n",
    "        if len(sen) != 0:\n",
    "            sentence = sen[0].strip()\n",
    "            list_all_word, list_word = morpho_j(sentence)\n",
    "            dict_sen_allword_word = {'sentence': sentence,\n",
    "                                     'word_all': list_all_word,\n",
    "                                     'word_nj': list_word}\n",
    "            list_master.append(dict_sen_allword_word)\n",
    "    Filer.writedump(list_master, './files/rakuten_ichiba/processedfile/morphofile/%s.dump' % num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_tmp = Filer.readdump('./files/rakuten_ichiba/processedfile/morphofile/1.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 採用したクラスタをtsvとして吐き出す\n",
    "list_cluster_num = [[0,1], [0,2], [0,4], [0,6], [1,1], \n",
    "                    [1,4], [1,5], [1,7], [2,2], [2,3],\n",
    "                    [3,2], [3,3], [4,3], [5,1], [5,6],\n",
    "                    [6,1], [6,5], [8,0], [8,4], [9,3],\n",
    "                    [12,3], [12,5], [14,2], [14,6], [15,2],\n",
    "                    [15,3], [20,2], [20,3], [20,5], [21,1],\n",
    "                    [21,3], [21,7], [25,2], [25,3], [27,0],\n",
    "                    [27,1], [27,2], [29,0], [29,3], [29,4]]\n",
    "\n",
    "for num0, num1 in list_cluster_num:\n",
    "    inputpath = './files/rakuten_ichiba/processedfile/classifiedfile/%s/cluster%s.dump' % (num0, num1)\n",
    "    list_dict = Filer.readdump(inputpath)\n",
    "    list_master = [[row['sentence']] for row in list_dict]\n",
    "    Filer.writetsv(list_master,\n",
    "                   './files/rakuten_ichiba/processedfile/classifiedfile/tsvfile/%s_%s.tsv'%(num0, num1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### goldのチェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_test = Filer.readdump('./files/OpinosisDataset1.0/preprocessed1/test/accuracy_garmin_nuvi_255W_gps.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_test['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_tmp = Filer.readdump('./files/OpinosisDataset1.0/preprocessed1/ans/accuracy_garmin_nuvi_255W_gps.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'This unit is generally quite accurate',\n",
       "  u'Set-up and usage are considered to be very easy',\n",
       "  u'The maps can be updated and tend to be reliable'],\n",
       " [u'Its accurate fast and its simple operations make this a for sure buy'],\n",
       " [u'It is very accurate even in destination time'],\n",
       " [u'The Garmin seems to be generally very accurate',\n",
       "  u\"It 's easy to use with an intuitive interface\"],\n",
       " [u'Very accurate with travel and destination time',\n",
       "  u'Negatives are not accurate with speed limits and rural roads']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_tmp['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### アンケートで表示するようのデータを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = ['../topic_model/files/rakuten_corpus/rakuten_corpus_master/txtfile/travel02_userReview05_20160304.txt',\n",
    "                 '../topic_model/files/rakuten_corpus/rakuten_corpus_master/txtfile/travel02_userReview06_20160304.txt',]\n",
    "list_fileeva = ['./files/rakuten/evaluation/txtfile/travel01_userEvaluation05_20160304.txt',\n",
    "                './files/rakuten/evaluation/txtfile/travel01_userEvaluation06_20160304.txt',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "list_file = [Filer.readtxt(path)  for path in list_filepath]\n",
    "# タブで区切る\n",
    "list_file = [sentence.split('\\t') for row in list_file for sentence in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 施設番号と年代で検索\n",
    "hotel_num = '54127'\n",
    "list_year = ['2010', '2011']\n",
    "list_file_rev = [row[0:4] for row in list_file if row[0]==hotel_num and row[1][0:4] in list_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1649\n"
     ]
    }
   ],
   "source": [
    "print len(list_file_rev)\n",
    "list_eva = [Filer.readtxt(path) for path in list_fileeva]\n",
    "list_eva = [sentence.split('\\t') for row in list_eva for sentence in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_eva = {row[0]:[row[2], row[7], row[8],\n",
    "                    row[9], row[10], row[11],\n",
    "                    row[12], row[13]] for row in list_eva}\n",
    "\n",
    "list_review_eva = [row+dict_eva[row[3]] for row in list_file_rev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Filer.writetsv(list_review_eva, './files/rakuten/txt_evaluation/txt_evaluation_1238.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['54127',\n",
       " '2010/01/01 15:24:28',\n",
       " '\\xe3\\x81\\x8a\\xe9\\x83\\xa8\\xe5\\xb1\\x8b\\xe3\\x81\\xae\\xe3\\x81\\x8a\\xe9\\xa2\\xa8\\xe5\\x91\\x82\\xe3\\x81\\x8c\\xe3\\x81\\xa8\\xe3\\x81\\xa6\\xe3\\x82\\x82\\xe5\\xba\\x83\\xe3\\x81\\x8f\\xe3\\x81\\xa6\\xe5\\xad\\x90\\xe4\\xbe\\x9b\\xe3\\x81\\xa8\\xe4\\xb8\\x80\\xe7\\xb7\\x92\\xe3\\x81\\xab\\xe3\\x82\\x86\\xe3\\x81\\xa3\\xe3\\x81\\x8f\\xe3\\x82\\x8a\\xe5\\x85\\xa5\\xe3\\x82\\x8c\\xe3\\x81\\xa6\\xe8\\x89\\xaf\\xe3\\x81\\x8b\\xe3\\x81\\xa3\\xe3\\x81\\x9f\\xe3\\x81\\xa7\\xe3\\x81\\x99\\xe3\\x80\\x82   \\xe3\\x80\\x90\\xe3\\x81\\x94\\xe5\\x88\\xa9\\xe7\\x94\\xa8\\xe3\\x81\\xae\\xe5\\xae\\xbf\\xe6\\xb3\\x8a\\xe3\\x83\\x97\\xe3\\x83\\xa9\\xe3\\x83\\xb3\\xe3\\x80\\x91 \\xe3\\x80\\x90\\xe3\\x81\\x8a\\xe6\\x97\\xa5\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe9\\x99\\x90\\xe5\\xae\\x9a\\xef\\xbc\\x81\\xe3\\x80\\x91\\xe4\\xbd\\x8e\\xe5\\xb1\\xa4\\xe9\\x9a\\x8e\\xe3\\x81\\xa0\\xe3\\x81\\x8b\\xe3\\x82\\x89\\xe3\\x81\\x8a\\xe5\\xbe\\x97\\xef\\xbc\\x81\\xe8\\x88\\x9e\\xe6\\xb5\\x9c\\xe3\\x82\\xb9\\xe3\\x83\\x9a\\xe3\\x82\\xb7\\xe3\\x83\\xa3\\xe3\\x83\\xab\\xef\\xbc\\x88\\xe3\\x81\\x8a\\xe5\\xbc\\x81\\xe5\\xbd\\x93\\xe4\\xbb\\x98\\xef\\xbc\\x89 \\xe3\\x81\\x8a\\xe9\\x83\\xa8\\xe5\\xb1\\x8b\\xe3\\x82\\xbf\\xe3\\x82\\xa4\\xe3\\x83\\x97\\xe3\\x81\\x8a\\xe3\\x81\\xbe\\xe3\\x81\\x8b\\xe3\\x81\\x9b\\xef\\xbc\\x92\\xe5\\x90\\x8d\\xef\\xbc\\x91\\xe5\\xae\\xa4',\n",
       " '7197620',\n",
       " 'user129413',\n",
       " '5',\n",
       " '4',\n",
       " '3',\n",
       " '4',\n",
       " '3',\n",
       " '3',\n",
       " '5']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_review_eva[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#【ご利用の宿泊プラン】~を削除\n",
    "pattern = r'【ご利用の宿泊プラン】.+$'\n",
    "list_review_eva1 = []\n",
    "for row in list_review_eva:\n",
    "    list_tmp = [row[3], row[4], row[1]]\n",
    "    list_tmp.append(re.sub(pattern, '', row[2]))\n",
    "    list_tmp = list_tmp + row[5:12]\n",
    "    list_review_eva1.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writetsv(list_review_eva1, './files/rakuten/txt_evaluation/txt_evaluation_1238_rev.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 読み込む形式に整形\n",
    "from datetime import datetime as dt\n",
    "list_review_eva1 = sorted(list_review_eva1, key=lambda x:x[2], reverse=True)\n",
    "list_eva = [{'user_id':int(list_review_eva1[num][0]), 'user_name':list_review_eva1[num][1],\n",
    "             'review':list_review_eva1[num][3], 'date_time':dt.strptime(list_review_eva1[num][2], '%Y/%m/%d %H:%M:%S'),\n",
    "             'eva1':int(list_review_eva1[num][4]),'eva2':int(list_review_eva1[num][5]),\n",
    "             'eva3':int(list_review_eva1[num][6]),'eva4':int(list_review_eva1[num][7]),\n",
    "             'eva5':int(list_review_eva1[num][8]),'eva6':int(list_review_eva1[num][9]),\n",
    "             'eva7':int(list_review_eva1[num][10])} for num in range(len(list_review_eva1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writedump(list_eva, './files/rakuten/txt_evaluation/txt_evaluation_54127.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1649"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_review_eva1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.50818677987\n",
      "4.63311097635\n",
      "2.70466949666\n",
      "4.53365676167\n",
      "4.56337174045\n",
      "4.42207398423\n",
      "4.61370527592\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,8):\n",
    "    counter = 0\n",
    "    for row in list_review_eva1:\n",
    "        counter+=int(row[3+i])\n",
    "    print float(counter)/len(list_review_eva1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_review = Filer.readdump('./files/rakuten/txt_evaluation/txt_evaluation_54127.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1649"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_all = []\n",
    "for row in list_review:\n",
    "    list_s = re.split(r'。|！', row['review'])\n",
    "    list_s = [sen for sen in list_s if len(sen) > 1]\n",
    "    for sen in list_s:\n",
    "        list_word_all, list_word = morpho_j(sen)\n",
    "        dict_tmp = {'sentence': sen,\n",
    "                    'user_name': row['user_name'],\n",
    "                    'date_time': row['date_time'],\n",
    "                    'sep_nj': list_word,\n",
    "                    'sep_all': list_word_all}\n",
    "        list_all.append(dict_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writedump(list_all, './files/rakuten/txt_evaluation/txt_sep_54127.dump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 分類表の整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_csv = Filer.readcsv('./files/dictionary/bunruidb/bunruidb.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_csv = [[row[7],\n",
    "             row[12].decode('shift-jis').encode('utf-8'),\n",
    "             row[13].decode('shift-jis').encode('utf-8'),] for row in list_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_word_class = {row[i]: row[0] for row in list_csv for i in range(1,3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writedump(dict_word_class, './files/dictionary/dict_word_class.dump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 極性辞書の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_txt = Filer.readtxt('./files/dictionary/pn_ja.txt', LF='\\r\\n')\n",
    "list_txt = [row.decode('shift-jis').encode('utf-8') for row in list_txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_word_pol = {}\n",
    "for row in list_txt:\n",
    "    list_row = row.split(':')\n",
    "    dict_word_pol[list_row[0]] = float(list_row[3])\n",
    "    if len(list_row[1].decode('utf-8')) > 1:\n",
    "        dict_word_pol[list_row[1]] = float(list_row[3])\n",
    "Filer.writedump(dict_word_pol, './files/dictionary/dict_word_pol.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_file = Filer.readdump('./files/rakuten/for_questionnaire/54127/txt_sep_54127.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_master = []\n",
    "for row in list_file:\n",
    "    dict_tmp = {}\n",
    "    list_sep_all, list_sep, list_sep_all_lemmas, list_sep_lemmas = morpho_j(row['sentence'])\n",
    "    dict_tmp['date_time'] = row['date_time']\n",
    "    dict_tmp['sentence'] = row['sentence']\n",
    "    dict_tmp['user_name'] = row['user_name']\n",
    "    dict_tmp['sep_all'] = list_sep_all\n",
    "    dict_tmp['sep_nj'] = list_sep\n",
    "    dict_tmp['sep_all_lemmas'] = list_sep_all_lemmas\n",
    "    dict_tmp['sep_nj_lemmas'] = list_sep_lemmas\n",
    "    list_master.append(dict_tmp)\n",
    "Filer.writedump(list_master, './files/rakuten/for_questionnaire/54127/txt_sep_54127.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
