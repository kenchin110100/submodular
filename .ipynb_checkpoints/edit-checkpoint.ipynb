{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"\n",
    "ファイルの編集を色々とするためのファイル\n",
    "\"\"\"\n",
    "from filer2.filer2 import Filer\n",
    "import numpy as np\n",
    "import glob\n",
    "import collections\n",
    "import re\n",
    "import MeCab\n",
    "import itertools\n",
    "import CaboCha\n",
    "from stanford_corenlp_pywrapper import CoreNLP\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CoreNLP_PyWrapper:Starting java subprocess, and waiting for signal it's ready, with command: exec java -Xmx4g -XX:ParallelGCThreads=1 -cp '/home/ikegami/ikegami/lib/python2.7/site-packages/stanford_corenlp_pywrapper/lib/*:/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*'      corenlp.SocketServer --outpipe /tmp/corenlp_pywrap_pipe_pypid=31807_time=1472456863.31  --configdict '{\"annotators\": \"tokenize, ssplit, pos, lemma, parse\"}'\n",
      "INFO:CoreNLP_PyWrapper:Successful ping. The server has started.\n",
      "INFO:CoreNLP_PyWrapper:Subprocess is ready.\n"
     ]
    }
   ],
   "source": [
    "# 関数の定義\n",
    "mecab = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "proc = CoreNLP(configdict={'annotators': 'tokenize, ssplit, pos, lemma, parse'},\n",
    "               corenlp_jars=[\"/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*\"])\n",
    "\n",
    "def morpho(sentence):\n",
    "    res = mecab.parseToNode(sentence)\n",
    "    list_words = []\n",
    "    while res:\n",
    "        features = res.feature.split(\",\")\n",
    "        if (features[0] == \"名詞\" and features[1] in [\"一般\", \"固有名詞\", \"サ変接続\", \"形容動詞語幹\"]) or features[0] == \"形容詞\":\n",
    "            if features[6] == \"*\":\n",
    "                if res.surface != '':\n",
    "                    list_words.append(res.surface)\n",
    "            else:\n",
    "                if features[6] != '':\n",
    "                    list_words.append(features[6])\n",
    "        res = res.next\n",
    "    return list_words\n",
    "\n",
    "def morpho_v(sentence):\n",
    "    res = mecab.parseToNode(sentence)\n",
    "    list_words = []\n",
    "    while res:\n",
    "        features = res.feature.split(\",\")\n",
    "        if (features[0] == \"名詞\" and features[1] in [\"一般\", \"固有名詞\", \"サ変接続\", \"形容動詞語幹\"]) or features[0] == \"形容詞\" or (features[0] == \"動詞\" and features[1] == \"自立\"):\n",
    "            if features[6] == \"*\":\n",
    "                if res.surface != '':\n",
    "                    list_words.append(res.surface)\n",
    "            else:\n",
    "                if features[6] != '':\n",
    "                    list_words.append(features[6])\n",
    "        res = res.next\n",
    "    return list_words\n",
    "\n",
    "def morpho1(sentence):\n",
    "    res = mecab.parseToNode(sentence)\n",
    "    list_words = []\n",
    "    while res:\n",
    "        features = res.feature.split(\",\")\n",
    "        if features[0] != \"記号\":\n",
    "            if features[6] == \"*\":\n",
    "                if res.surface != '':\n",
    "                    list_words.append(res.surface)\n",
    "            else:\n",
    "                if features[6] != '':\n",
    "                    list_words.append(features[6])\n",
    "        res = res.next\n",
    "    return list_words\n",
    "\n",
    "cp = CaboCha.Parser('-f1 -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "\n",
    "def get_v_word(tree, chunk):\n",
    "    surface = []\n",
    "    for i in range(chunk.token_pos, chunk.token_pos + chunk.token_size):\n",
    "        token = tree.token(i)\n",
    "        features = token.feature.split(',')\n",
    "        if features[0] == '名詞' and features[1] in [\"一般\", \"固有名詞\", \"サ変接続\", \"形容動詞語幹\"]:\n",
    "            surface.append(token.surface)\n",
    "        elif features[0] == '形容詞':\n",
    "            surface.append(features[6])\n",
    "        elif features[0] == '動詞' and features[1] == '自立':\n",
    "            surface.append(features[6])\n",
    "    return tuple(surface)\n",
    "\n",
    "def get_v_words(line):\n",
    "    cp = CaboCha.Parser('-f1 -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "    tree = cp.parse(line)\n",
    "    chunk_dic = {}\n",
    "    chunk_id = 0\n",
    "    for i in range(0, tree.size()):\n",
    "        token = tree.token(i)\n",
    "        if token.chunk:\n",
    "            chunk_dic[chunk_id] = token.chunk\n",
    "            chunk_id += 1\n",
    "\n",
    "    tuples = []\n",
    "    for chunk_id, chunk in chunk_dic.items():\n",
    "        if chunk.link > 0:\n",
    "            from_surface =  get_v_word(tree, chunk)\n",
    "            to_chunk = chunk_dic[chunk.link]\n",
    "            to_surface = get_v_word(tree, to_chunk)\n",
    "            tuples.extend(itertools.product(from_surface, to_surface))\n",
    "    return tuples\n",
    "\n",
    "def get_word(tree, chunk):\n",
    "    surface = []\n",
    "    for i in range(chunk.token_pos, chunk.token_pos + chunk.token_size):\n",
    "        token = tree.token(i)\n",
    "        features = token.feature.split(',')\n",
    "        if features[0] == '名詞' and features[1] in [\"一般\", \"固有名詞\", \"サ変接続\", \"形容動詞語幹\"]:\n",
    "            surface.append(token.surface)\n",
    "        elif features[0] == '形容詞':\n",
    "            surface.append(features[6])\n",
    "    return tuple(surface)\n",
    "\n",
    "def get_words(line):\n",
    "    cp = CaboCha.Parser('-f1 -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "    tree = cp.parse(line)\n",
    "    chunk_dic = {}\n",
    "    chunk_id = 0\n",
    "    for i in range(0, tree.size()):\n",
    "        token = tree.token(i)\n",
    "        if token.chunk:\n",
    "            chunk_dic[chunk_id] = token.chunk\n",
    "            chunk_id += 1\n",
    "\n",
    "    tuples = []\n",
    "    for chunk_id, chunk in chunk_dic.items():\n",
    "        if chunk.link > 0:\n",
    "            from_surface =  get_word(tree, chunk)\n",
    "            to_chunk = chunk_dic[chunk.link]\n",
    "            to_surface = get_word(tree, to_chunk)\n",
    "            tuples.extend(itertools.product(from_surface, to_surface))\n",
    "    return tuples\n",
    "\n",
    "p_n = r'^NN'\n",
    "p_v = r'^VB'\n",
    "p_j = r'^JJ'\n",
    "\n",
    "def morpho_e_njv(sentence):\n",
    "    list_sentence = proc.parse_doc(sentence)['sentences']\n",
    "    list_all_bag = []\n",
    "    list_all_edge = []\n",
    "    list_all_sentence = []\n",
    "    list_all_lemmas = []\n",
    "    for dict_tmp in list_sentence:\n",
    "        sentence = [word for word in dict_tmp['tokens']\n",
    "                    if word != ',' and word != '.' and word != ' ']\n",
    "        list_all_sentence.append(' '.join(sentence))\n",
    "        list_lemmas = dict_tmp['tokens']\n",
    "        list_deps = dict_tmp['deps_cc']\n",
    "        list_pos = dict_tmp['pos']\n",
    "        dict_key_lemmas = {}\n",
    "        dict_lemmas_key = {}\n",
    "        list_bag = []\n",
    "        for i, row in enumerate(zip(list_lemmas, list_pos)):\n",
    "            lemmas = row[0]\n",
    "            pos = row[1]\n",
    "\n",
    "            if re.match(p_n, pos) or re.match(p_j, pos) or re.match(p_v, pos):\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "            \n",
    "        list_edge = []\n",
    "        for deps in list_deps:\n",
    "            if deps[1] in dict_key_lemmas and deps[2] in dict_key_lemmas:\n",
    "                list_edge.append([dict_key_lemmas[deps[1]],\n",
    "                                  dict_key_lemmas[deps[2]]])\n",
    "\n",
    "        list_all_bag.append(list_bag)\n",
    "        list_all_edge.append(list_edge)\n",
    "    return list_all_sentence, list_all_bag, list_all_edge\n",
    "\n",
    "def morpho_e(sentence):\n",
    "    list_sentence = proc.parse_doc(sentence)['sentences']\n",
    "    list_all_bag = []\n",
    "    list_all_edge = []\n",
    "    list_all_sentence = []\n",
    "    for dict_tmp in list_sentence:\n",
    "        sentence = [word for word in dict_tmp['tokens']\n",
    "                    if word != ',' and word != '.' and word != ' ']\n",
    "        list_all_sentence.append(' '.join(sentence))\n",
    "        list_lemmas = dict_tmp['tokens']\n",
    "        list_deps = dict_tmp['deps_cc']\n",
    "        list_pos = dict_tmp['pos']\n",
    "        dict_key_lemmas = {}\n",
    "        dict_lemmas_key = {}\n",
    "        list_bag = []\n",
    "        for i, row in enumerate(zip(list_lemmas, list_pos)):\n",
    "            lemmas = row[0]\n",
    "            pos = row[1]\n",
    "\n",
    "            if lemmas != ',' and lemmas != ' ' and lemmas != '.':\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "            \n",
    "        list_edge = []\n",
    "        for deps in list_deps:\n",
    "            if deps[1] in dict_key_lemmas and deps[2] in dict_key_lemmas:\n",
    "                list_edge.append([dict_key_lemmas[deps[1]],\n",
    "                                  dict_key_lemmas[deps[2]]])\n",
    "\n",
    "        list_all_bag.append(list_bag)\n",
    "        list_all_edge.append(list_edge)\n",
    "    return list_all_sentence, list_all_bag, list_all_edge\n",
    "\n",
    "def morpho_e_nj(sentence):\n",
    "    list_sentence = proc.parse_doc(sentence)['sentences']\n",
    "    list_all_bag = []\n",
    "    list_all_edge = []\n",
    "    list_all_sentence = []\n",
    "    for dict_tmp in list_sentence:\n",
    "        sentence = [word for word in dict_tmp['tokens']\n",
    "                    if word != ',' and word != '.' and word != ' ']\n",
    "        list_all_sentence.append(' '.join(sentence))\n",
    "        list_lemmas = dict_tmp['tokens']\n",
    "        list_deps = dict_tmp['deps_cc']\n",
    "        list_pos = dict_tmp['pos']\n",
    "        dict_key_lemmas = {}\n",
    "        dict_lemmas_key = {}\n",
    "        list_bag = []\n",
    "        for i, row in enumerate(zip(list_lemmas, list_pos)):\n",
    "            lemmas = row[0]\n",
    "            pos = row[1]\n",
    "\n",
    "            if re.match(p_n, pos) or re.match(p_j, pos):\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "            \n",
    "        list_edge = []\n",
    "        for deps in list_deps:\n",
    "            if deps[1] in dict_key_lemmas and deps[2] in dict_key_lemmas:\n",
    "                list_edge.append([dict_key_lemmas[deps[1]],\n",
    "                                  dict_key_lemmas[deps[2]]])\n",
    "\n",
    "        list_all_bag.append(list_bag)\n",
    "        list_all_edge.append(list_edge)\n",
    "    return list_all_sentence, list_all_bag, list_all_edge\n",
    "\n",
    "\n",
    "def morpho_e_njv_pos(sentence):\n",
    "    list_sentence = proc.parse_doc(sentence)['sentences']\n",
    "    list_all_bag = []\n",
    "    list_all_edge = []\n",
    "    list_all_sentence = []\n",
    "    list_all_lemmas = []\n",
    "    list_all_pos = []\n",
    "    for dict_tmp in list_sentence:\n",
    "        sentence = [word for word in dict_tmp['tokens']\n",
    "                    if word != ',' and word != '.' and word != ' ']\n",
    "        list_all_sentence.append(' '.join(sentence))\n",
    "        list_lemmas = dict_tmp['tokens']\n",
    "        list_deps = dict_tmp['deps_cc']\n",
    "        list_pos = dict_tmp['pos']\n",
    "        dict_key_lemmas = {}\n",
    "        dict_lemmas_key = {}\n",
    "        list_bag = []\n",
    "        for i, row in enumerate(zip(list_lemmas, list_pos)):\n",
    "            lemmas = row[0]\n",
    "            pos = row[1]\n",
    "\n",
    "            if re.match(p_n, pos):\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "                list_all_pos.append('n')\n",
    "            elif re.match(p_j, pos):\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "                list_all_pos.append('j')\n",
    "            elif re.match(p_v, pos):\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "                list_all_pos.append('v')\n",
    "            \n",
    "        list_edge = []\n",
    "        for deps in list_deps:\n",
    "            if deps[1] in dict_key_lemmas and deps[2] in dict_key_lemmas:\n",
    "                list_edge.append([dict_key_lemmas[deps[1]],\n",
    "                                  dict_key_lemmas[deps[2]]])\n",
    "\n",
    "        list_all_bag.append(list_bag)\n",
    "        list_all_edge.append(list_edge)\n",
    "    return list_all_sentence, list_all_bag, list_all_edge, list_all_pos\n",
    "\n",
    "def morpho_e_pos(sentence):\n",
    "    list_sentence = proc.parse_doc(sentence)['sentences']\n",
    "    list_all_bag = []\n",
    "    list_all_edge = []\n",
    "    list_all_sentence = []\n",
    "    list_all_pos = []\n",
    "    for dict_tmp in list_sentence:\n",
    "        sentence = [word for word in dict_tmp['tokens']\n",
    "                    if word != ',' and word != '.' and word != ' ']\n",
    "        list_all_sentence.append(' '.join(sentence))\n",
    "        list_lemmas = dict_tmp['tokens']\n",
    "        list_deps = dict_tmp['deps_cc']\n",
    "        list_pos = dict_tmp['pos']\n",
    "        dict_key_lemmas = {}\n",
    "        dict_lemmas_key = {}\n",
    "        list_bag = []\n",
    "        for i, row in enumerate(zip(list_lemmas, list_pos)):\n",
    "            lemmas = row[0]\n",
    "            pos = row[1]\n",
    "            if re.match(p_n, pos):\n",
    "                list_all_pos.append('n')\n",
    "            elif re.match(p_j, pos):\n",
    "                list_all_pos.append('j')\n",
    "            elif re.match(p_v, pos):\n",
    "                list_all_pos.append('v')\n",
    "            else:\n",
    "                list_all_pos.append('-')\n",
    "\n",
    "            if lemmas != ',' and lemmas != ' ' and lemmas != '.':\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "            \n",
    "        list_edge = []\n",
    "        for deps in list_deps:\n",
    "            if deps[1] in dict_key_lemmas and deps[2] in dict_key_lemmas:\n",
    "                list_edge.append([dict_key_lemmas[deps[1]],\n",
    "                                  dict_key_lemmas[deps[2]]])\n",
    "\n",
    "        list_all_bag.append(list_bag)\n",
    "        list_all_edge.append(list_edge)\n",
    "    return list_all_sentence, list_all_bag, list_all_edge, list_all_pos\n",
    "\n",
    "def morpho_e_nj_pos(sentence):\n",
    "    list_sentence = proc.parse_doc(sentence)['sentences']\n",
    "    list_all_bag = []\n",
    "    list_all_edge = []\n",
    "    list_all_sentence = []\n",
    "    list_all_pos = []\n",
    "    for dict_tmp in list_sentence:\n",
    "        sentence = [word for word in dict_tmp['tokens']\n",
    "                    if word != ',' and word != '.' and word != ' ']\n",
    "        list_all_sentence.append(' '.join(sentence))\n",
    "        list_lemmas = dict_tmp['tokens']\n",
    "        list_deps = dict_tmp['deps_cc']\n",
    "        list_pos = dict_tmp['pos']\n",
    "        dict_key_lemmas = {}\n",
    "        dict_lemmas_key = {}\n",
    "        list_bag = []\n",
    "        for i, row in enumerate(zip(list_lemmas, list_pos)):\n",
    "            lemmas = row[0]\n",
    "            pos = row[1]\n",
    "\n",
    "            if re.match(p_n, pos):\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "                list_all_pos.append('n')\n",
    "            elif re.match(p_j, pos):\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "                list_all_pos.append('j')\n",
    "            \n",
    "        list_edge = []\n",
    "        for deps in list_deps:\n",
    "            if deps[1] in dict_key_lemmas and deps[2] in dict_key_lemmas:\n",
    "                list_edge.append([dict_key_lemmas[deps[1]],\n",
    "                                  dict_key_lemmas[deps[2]]])\n",
    "\n",
    "        list_all_bag.append(list_bag)\n",
    "        list_all_edge.append(list_edge)\n",
    "    return list_all_sentence, list_all_bag, list_all_edge, list_all_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_sentence, list_lemmas, list_bag, list_edge = morpho_e('The clusters of review sentences on the viewpoints from the products evaluation can be applied to various use. The topic models, for example Unigram Mixture (UM), can be used for this task.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'The', u'clusters', u'of', u'review', u'sentences', u'on', u'the', u'viewpoints', u'from', u'the', u'products', u'evaluation', u'can', u'be', u'applied', u'to', u'various', u'use'] [u'the', u'cluster', u'of', u'review', u'sentence', u'on', u'the', u'viewpoint', u'from', u'the', u'product', u'evaluation', u'can', u'be', u'apply', u'to', u'various', u'use'] ['cluster', 'review', 'sentence', 'viewpoint', 'product', 'evaluation', 'various', 'use'] [['cluster', 'sentence'], ['sentence', 'review'], ['sentence', 'viewpoint'], ['viewpoint', 'evaluation'], ['evaluation', 'product'], ['use', 'various']]\n",
      "[u'The', u'topic', u'models', u'for', u'example', u'Unigram', u'Mixture', u'-LRB-', u'UM', u'-RRB-', u'can', u'be', u'used', u'for', u'this', u'task'] [u'the', u'topic', u'model', u'for', u'example', u'Unigram', u'Mixture', u'-lrb-', u'UM', u'-rrb-', u'can', u'be', u'use', u'for', u'this', u'task'] ['topic', 'model', 'example', 'Unigram', 'Mixture', 'UM', 'task'] [['model', 'Mixture'], ['model', 'topic'], ['Mixture', 'example'], ['Mixture', 'Unigram'], ['Mixture', 'UM']]\n"
     ]
    }
   ],
   "source": [
    "for sentence, lemmas, bag, edge in zip(list_sentence, list_lemmas, list_bag, list_edge):\n",
    "    print sentence, lemmas, bag, edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全体のコーパスを施設別に集計する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = glob.glob('../topic_model/files/rakuten_corpus/rakuten_corpus_master/txtfile/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "list_file = [Filer.readtxt(path)  for path in list_filepath]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# タブで区切る\n",
    "list_file = [[sentence.split('\\t') for sentence in row] for row in list_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ホテル番号別に集計\n",
    "dict_hotel_sentence = collections.defaultdict(list)\n",
    "for row in list_file:\n",
    "    for sen in row:\n",
    "        dict_hotel_sentence[sen[0]].append(sen[1] + '\\t' + sen[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# review数が100件未満のホテルは除外して保存する\n",
    "for key, value in dict_hotel_sentence.items():\n",
    "    if len(value) >= 100:\n",
    "        Filer.writetxt(value, '../files/txtfile/review-'+key+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数を集計\n",
    "dict_count = {key: len(value) for key, value in dict_hotel_sentence.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 施設名と番号を合わせる\n",
    "list_name = Filer.readtxt('../files/travel03_hotelMaster_20160304.txt')\n",
    "list_name = [sen.split('\\t') for sen in list_name]\n",
    "dict_num_name = {row[0]: row[1] for row in list_name if len(row) == 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 年別にレビュー数を集計\n",
    "dict_counter = {}\n",
    "for key, value in dict_hotel_sentence.items():\n",
    "    dict_counter_tmp = collections.defaultdict(int)\n",
    "    for sen in value:\n",
    "        dict_counter_tmp[sen[0:4]] += 1\n",
    "    dict_counter[key] = dict_counter_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'2001': 3,\n",
       "             '2002': 6,\n",
       "             '2003': 17,\n",
       "             '2004': 6,\n",
       "             '2005': 54,\n",
       "             '2006': 288,\n",
       "             '2007': 96,\n",
       "             '2008': 42,\n",
       "             '2009': 38,\n",
       "             '2010': 31,\n",
       "             '2011': 11})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_count['9680']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 施設番号、施設名、レビュー数のファイルを作成する\n",
    "list_master = [['施設番号', '施設名', 'レビュー数',\n",
    "                '1999', '2000', '2001', '2002',\n",
    "                '2003', '2004', '2005', '2006',\n",
    "                '2007', '2008', '2009', '2010',\n",
    "                '2011']]\n",
    "for key, value in sorted(dict_count.items(), key=lambda x:x[1], reverse=True):\n",
    "    if key in dict_num_name:\n",
    "        list_master.append([key, dict_num_name[key], value, dict_counter[key]['1999'],\n",
    "                            dict_counter[key]['2000'], dict_counter[key]['2001'], dict_counter[key]['2002'],\n",
    "                            dict_counter[key]['2003'], dict_counter[key]['2004'], dict_counter[key]['2005'],\n",
    "                            dict_counter[key]['2006'], dict_counter[key]['2007'], dict_counter[key]['2008'],\n",
    "                            dict_counter[key]['2009'], dict_counter[key]['2010'], dict_counter[key]['2011']])\n",
    "    else:\n",
    "        list_master.append([key, '', value, dict_counter[key]['1999'],\n",
    "                            dict_counter[key]['2000'], dict_counter[key]['2001'], dict_counter[key]['2002'],\n",
    "                            dict_counter[key]['2003'], dict_counter[key]['2004'], dict_counter[key]['2005'],\n",
    "                            dict_counter[key]['2006'], dict_counter[key]['2007'], dict_counter[key]['2008'],\n",
    "                            dict_counter[key]['2009'], dict_counter[key]['2010'], dict_counter[key]['2011']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# csvfileとして保存\n",
    "Filer.writecsv(list_master, '../files/txtfile/master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析対象とするホテルのみを取り出して、sentenceごとのファイルにする\n",
    "* 51637：アパホテル＆リゾート＜東京ベイ幕張＞\n",
    "* 1238：オリエンタルホテル東京ベイ\n",
    "* 53171：加賀の湧泉　ドーミーイン金沢\n",
    "* 19191：新横浜プリンスホテル\n",
    "* 76887：ホテルモントレ　グラスミア大阪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = ['../files/txtfile/review-51637.txt',\n",
    "                 '../files/txtfile/review-1238.txt',\n",
    "                 '../files/txtfile/review-53171.txt',\n",
    "                 '../files/txtfile/review-19191.txt',\n",
    "                 '../files/txtfile/review-76887.txt',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2010年、2011年の文章のみを取得\n",
    "list_review_rev = []\n",
    "for path in list_filepath:\n",
    "    list_review = Filer.readtxt(path)\n",
    "    list_tmp = []\n",
    "    for review in list_review:\n",
    "        if review[0:4] == '2010' or review[0:4] == '2011':\n",
    "            list_tmp.append(review.split('\\t')[1])\n",
    "    list_review_rev.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#【ご利用の宿泊プラン】~を削除\n",
    "pattern = r'【ご利用の宿泊プラン】.+$'\n",
    "list_review_rev1 = []\n",
    "for row in list_review_rev:\n",
    "    list_tmp = []\n",
    "    for sentence in row:\n",
    "        list_tmp.append(re.sub(pattern, '', sentence))\n",
    "    list_review_rev1.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sentenceに変更、その際、ユーザー番号を頭に振る\n",
    "list_sentence = []\n",
    "for row in list_review_rev1:\n",
    "    list_tmp = []\n",
    "    for i, review in enumerate(row):\n",
    "        list_s = re.split(r'。|！|？', review)\n",
    "        for sen in list_s:\n",
    "            list_tmp.append([i, sen])\n",
    "    list_sentence.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 文字の前後の空白を削除\n",
    "list_sentence_rev = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        sentence = sen[1].replace(' ', '')\n",
    "        if len(sentence) > 1:\n",
    "            list_tmp.append([sen[0], sentence])\n",
    "    list_sentence_rev.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 先頭が、」もしくは）で始まってる場合、前の文にくっつける\n",
    "list_sentence_rev1 = []\n",
    "for row in list_sentence_rev:\n",
    "    list_tmp = []\n",
    "    for i in range(len(row)-1):\n",
    "        if re.match(r'」|』|\\)|）', row[i][1]):\n",
    "            pass\n",
    "        elif re.match(r'」|』|\\)|）', row[i+1][1]):\n",
    "            list_tmp.append([row[i][0], row[i][1]+row[i+1][1]])\n",
    "        else:\n",
    "            list_tmp.append([row[i][0], row[i][1]])\n",
    "    else:\n",
    "        list_tmp.append([row[i+1][0], row[i+1][1]])\n",
    "        list_sentence_rev1.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Filer.writetsv(list_sentence_rev1[0], '../files/sentencefile/sentence-51637.tsv')\n",
    "Filer.writetsv(list_sentence_rev1[1], '../files/sentencefile/sentence-1238.tsv')\n",
    "Filer.writetsv(list_sentence_rev1[2], '../files/sentencefile/sentence-53171.tsv')\n",
    "Filer.writetsv(list_sentence_rev1[3], '../files/sentencefile/sentence-19191.tsv')\n",
    "Filer.writetsv(list_sentence_rev1[4], '../files/sentencefile/sentence-76887.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### sentenceごとのファイルを形態素解析済みのファイルに変更する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../files/sentencefile/sentence-76887.tsv', '../files/sentencefile/sentence-1238.tsv', '../files/sentencefile/sentence-53171.tsv', '../files/sentencefile/sentence-19191.tsv', '../files/sentencefile/sentence-51637.tsv']\n"
     ]
    }
   ],
   "source": [
    "list_path = glob.glob('../files/sentencefile/*')\n",
    "print list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentence = [Filer.readtsv(path) for path in list_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_morpho = [[[sen[1], morpho(sen[1])] for sen in row]for row in list_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writedump(list_morpho[4], '../files/morphofile/type1/morpho-51637.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_morpho1 = [[[sen[1], morpho1(sen[1])] for sen in row] for row in list_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writedump(list_morpho1[4], '../files/morphofile/type2/morpho-51637.dump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高頻度語、低頻度語を数え上げて削除したファイルを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_word = [word for row in list_morpho for sen in row for word in sen[1]]\n",
    "list_word1 = [word for row in list_morpho1 for sen in row for word in sen[1]]\n",
    "dict_word_freq = collections.Counter(list_word)\n",
    "dict_word_freq1 = collections.Counter(list_word1)\n",
    "list_set_word = []\n",
    "for row in list_morpho:\n",
    "    for sen in row:\n",
    "        list_set_word.extend(list(set(sen[1])))\n",
    "        \n",
    "list_set_word1 = []\n",
    "for row in list_morpho1:\n",
    "    for sen in row:\n",
    "        list_set_word1.extend(list(set(sen[1])))\n",
    "\n",
    "dict_document_freq = collections.Counter(list_set_word)\n",
    "dict_document_freq1 = collections.Counter(list_set_word1)\n",
    "print len(list_morpho[0])+len(list_morpho[1])+len(list_morpho[2])+len(list_morpho[3])+len(list_morpho[4])\n",
    "print len(list_morpho1[0])+len(list_morpho1[1])+len(list_morpho1[2])+len(list_morpho1[3])+len(list_morpho1[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "部屋 0.166852010265\n",
      "利用 0.136783575706\n",
      "ホテル 0.100427715997\n",
      "良い 0.0860992301112\n",
      "宿泊 0.0706159110351\n",
      "満足 0.0563301967494\n",
      "便利 0.047005988024\n",
      "朝食 0.0458083832335\n",
      "よい 0.0455089820359\n",
      "広い 0.0444824636441\n"
     ]
    }
   ],
   "source": [
    "# 頻度の確認、10%切りで良さそう\n",
    "for row in sorted(dict_document_freq.items(), key=lambda x:x[1], reverse=True)[0:10]:\n",
    "    print row[0], row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 除去語の記録、10%以上、2回以下\n",
    "list_remove = []\n",
    "for row in dict_document_freq.items():\n",
    "    if row[1] >= 0.1:\n",
    "        list_remove.append(row[0])\n",
    "for row in dict_word_freq.items():\n",
    "    if row[1] <= 2:\n",
    "        list_remove.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Filer.writetxt(list_remove, '../files/preprocessedfile/dict/remove_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../files/morphofile/type1/morpho-51637.dump',\n",
       " '../files/morphofile/type1/morpho-53171.dump',\n",
       " '../files/morphofile/type1/morpho-1238.dump',\n",
       " '../files/morphofile/type1/morpho-76887.dump',\n",
       " '../files/morphofile/type1/morpho-19191.dump']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_path = glob.glob('../files/morphofile/type1/*.dump')\n",
    "list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentence = [Filer.readdump(path) for path in list_path]\n",
    "list_sentence_rev = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        list_tmp_tmp = [word for word in sen[1] if word not in list_remove]\n",
    "        list_tmp.append([sen[0], list_tmp_tmp])\n",
    "    list_sentence_rev.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 総単語数が１以下の文を削除\n",
    "list_sentence_rev = [[sen for sen in row if len(sen[1]) >= 2] for row in list_sentence_rev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writedump(list_sentence_rev[4], '../files/preprocessedfile/type11/wordlist/preprocessed-19191.dump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エッジリストの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../files/preprocessedfile/type11/wordlist/preprocessed-19191.dump',\n",
       " '../files/preprocessedfile/type11/wordlist/preprocessed-76887.dump',\n",
       " '../files/preprocessedfile/type11/wordlist/preprocessed-1238.dump',\n",
       " '../files/preprocessedfile/type11/wordlist/preprocessed-51637.dump',\n",
       " '../files/preprocessedfile/type11/wordlist/preprocessed-53171.dump']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_path = glob.glob('../files/preprocessedfile/type11/wordlist/*')\n",
    "list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentence = [Filer.readdump(path) for path in list_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bigram\n",
    "list_bigram = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        for i in range(len(sen[1])-1):\n",
    "            list_tmp.append([sen[1][i], sen[1][i+1]])\n",
    "    list_bigram.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writetsv(list_bigram[4], '../files/preprocessedfile/type11/bigram/bigram-53171.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cor\n",
    "list_cor = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        list_tmp.extend(list(itertools.combinations(tuple(sen[1]),2)))\n",
    "    list_tmp = [list(row) for row in list_tmp]\n",
    "    list_cor.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writetsv(list_cor[4], '../files/preprocessedfile/type11/cor/cor-53171.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'b'),\n",
       " ('a', 'c'),\n",
       " ('a', 'd'),\n",
       " ('a', 'e'),\n",
       " ('b', 'c'),\n",
       " ('b', 'd'),\n",
       " ('b', 'e'),\n",
       " ('c', 'd'),\n",
       " ('c', 'e'),\n",
       " ('d', 'e')]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = ['a', 'b', 'c', 'd', 'e']\n",
    "list(itertools.combinations(tuple(seq),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### PRTMによって、分類した後の文書を形態素解析する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_path = glob.glob('./files/classifiedfile/type11/bigram/sentence/*')\n",
    "removeword = './files/classifiedfile/type11/bigram/sentence/classified'\n",
    "list_name = [path.replace(removeword, '') for path in list_path]\n",
    "list_name = [name.replace('.txt', '') for name in list_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "list_sentence = [Filer.readtxt(path) for path in list_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 形態素解析\n",
    "list_master = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = [morpho_v(sentence) for sentence in row]\n",
    "    list_master.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "for row, name in zip(list_master, list_name):\n",
    "    Filer.writetsv(row, './files/classifiedfile/type11/bigram/morpho/n_adj_verb/classified'+name+'.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# エッジリストの作成\n",
    "list_bigram = []\n",
    "for row in list_master:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        for i in range(len(sen)-1):\n",
    "            list_tmp.append([sen[i], sen[i+1]])\n",
    "    list_bigram.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "for row, name in zip(list_bigram, list_name):\n",
    "    Filer.writetsv(row, './classifiedfile/type11/bigram/bigram/classified'+name+'.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cor\n",
    "list_cor = []\n",
    "for row in list_master:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        list_tmp.extend(list(itertools.combinations(tuple(sen),2)))\n",
    "    list_tmp = [list(row) for row in list_tmp]\n",
    "    list_cor.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "for row, name in zip(list_cor, list_name):\n",
    "    Filer.writetsv(row, './classifiedfile/type11/bigram/cor/classified'+name+'.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 構文構造を利用してエッジリストを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_path = glob.glob('files/classifiedfile/type11/bigram/sentence/classified*')\n",
    "removeword = 'files/classifiedfile/type11/bigram/sentence/classified'\n",
    "list_name = [path.replace(removeword, '') for path in list_path]\n",
    "list_name = [name.replace('.txt', '') for name in list_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "list_sentence = [Filer.readtxt(path) for path in list_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 構文によるエッジリストの作成(動詞なし)\n",
    "list_master = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = [[t[0], t[1]] for sentence in row for t in get_words(sentence)]\n",
    "    list_master.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開業 => 新横浜\n",
      "開業 => プリンス\n",
      "駅 => 近い\n",
      "近い => 近い\n",
      "アリーナ => 近い\n",
      "近い => 利用\n",
      "新横浜 => イベント\n",
      "横浜アリーナ => ライヴ\n",
      "ライヴ => あり\n",
      "ライヴ => 利用\n"
     ]
    }
   ],
   "source": [
    "for row in list_master[0][0:10]:\n",
    "    print row[0], '=>', row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "for row, name in zip(list_master, list_name):\n",
    "    Filer.writetsv(row, './files/classifiedfile/type11/bigram/syntax/non_verb/classified'+name+'.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 構文によるエッジリストの作成(動詞あり)\n",
    "list_master = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = [[t[0], t[1]] for sentence in row for t in get_v_words(sentence)]\n",
    "    list_master.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "for row, name in zip(list_master, list_name):\n",
    "    Filer.writetsv(row, './files/classifiedfile/type11/bigram/syntax/verb/classified'+name+'.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "フロント  =>  配置\n",
      "フロント  =>  欲しい\n",
      "人  =>  配置\n",
      "人  =>  欲しい\n"
     ]
    }
   ],
   "source": [
    "sentence = '時間的に混むのがわかったいるんだから、もう少しフロントに人を配置して欲しい'\n",
    "t = get_words(sentence)\n",
    "for row in t:\n",
    "    print row[0], ' => ', row[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### OpinosisDatasetの基礎集計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "list_path_corpus = glob.glob('./files/OpinosisDataset1.0/topics/*')\n",
    "list_path_gold = glob.glob('./files/OpinosisDataset1.0/summaries-gold/*')\n",
    "print len(list_path_corpus)\n",
    "print len(list_path_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Garmin seems to be generally very accurate.',\n",
       " \"It's easy to use with an intuitive interface.\"]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_corpus_gold = Filer.readtxt('./files/OpinosisDataset1.0/summaries-gold/accuracy_garmin_nuvi_255W_gps/accuracy_garmin_nuvi_255W_gps.2.gold', LF='\\r\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpinosisDatasetの形態素解析\n",
    "1. トピックごとにdictを作成する\n",
    " * sentence: 元の文のリスト\n",
    " * sep_all: 含まれているすべての単語のリスト\n",
    " * sep_nj: 含まれている名詞と形容詞のリスト\n",
    " * sep_njv: 含まれている名詞と形容詞と動詞のリスト\n",
    " * edge_nj: すべての単語の構文によるエッジ\n",
    " * edge_nj: 名詞と形容詞の構文によるエッジ\n",
    " * edge_njv: 名詞と形容詞と動詞の構文によるエッジ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_path = glob.glob('./files/OpinosisDataset1.0/topics/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CoreNLP_PyWrapper:Starting java subprocess, and waiting for signal it's ready, with command: exec java -Xmx4g -XX:ParallelGCThreads=1 -cp '/home/ikegami/ikegami/lib/python2.7/site-packages/stanford_corenlp_pywrapper/lib/*:/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*'      corenlp.SocketServer --outpipe /tmp/corenlp_pywrap_pipe_pypid=12569_time=1472178902.01  --configdict '{\"annotators\": \"tokenize, ssplit, pos, lemma, parse\"}'\n",
      "INFO:CoreNLP_PyWrapper:Successful ping. The server has started.\n",
      "INFO:CoreNLP_PyWrapper:Subprocess is ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./files/OpinosisDataset1.0/preprocessed1/gas_mileage_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/location_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/interior_honda_accord_2008.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/sound_ipod_nano_8gb.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/size_asus_netbook_1005ha.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/eyesight-issues_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/display_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/video_ipod_nano_8gb.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/buttons_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/screen_ipod_nano_8gb.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/location_holiday_inn_london.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/comfort_honda_accord_2008.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/service_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/features_windows7.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/fonts_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/bathroom_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/mileage_honda_accord_2008.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/screen_netbook_1005ha.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/speed_windows7.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/interior_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/staff_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/seats_honda_accord_2008.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/food_swissotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/quality_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/parking_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/voice_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/battery-life_ipod_nano_8gb.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/service_swissotel_hotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/screen_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/directions_garmin_nuvi_255W_gps.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/free_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/speed_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/navigation_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/satellite_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/performance_netbook_1005ha.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/price_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/transmission_toyota_camry_2007.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/comfort_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/food_holiday_inn_london.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/updates_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/accuracy_garmin_nuvi_255W_gps.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/price_holiday_inn_london.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/service_holiday_inn_london.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/battery-life_netbook_1005ha.dump\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/rooms_swissotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/keyboard_netbook_1005ha.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/battery-life_amazon_kindle.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/room_holiday_inn_london.dump\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/rooms_bestwestern_hotel_sfo.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed1/staff_swissotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed1/performance_honda_accord_2008.dump\n"
     ]
    }
   ],
   "source": [
    "proc = CoreNLP(configdict={'annotators': 'tokenize, ssplit, pos, lemma, parse'},\n",
    "               corenlp_jars=[\"/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*\"])\n",
    "\n",
    "for path in list_path:\n",
    "    dict_total = {'sentence':[],\n",
    "                  'sep_all':[],\n",
    "                  'sep_nj':[],\n",
    "                  'sep_njv':[],\n",
    "                  'edge_all': [],\n",
    "                  'edge_nj':[],\n",
    "                  'edge_njv': []}\n",
    "    filename = path.replace('./files/OpinosisDataset1.0/topics/', '').replace('.txt.data', '')\n",
    "    outputpath = './files/OpinosisDataset1.0/preprocessed1/'+filename+'.dump'\n",
    "    list_corpus = Filer.readtxt(path, LF='\\r\\n')\n",
    "    for corpus in list_corpus:\n",
    "        try:\n",
    "            list_sentence_a, list_bag_a, list_edge_a = morpho_e(corpus)\n",
    "            list_sentence_e, list_bag_e, list_edge_e = morpho_e_nj(corpus)\n",
    "            list_sentence_v, list_bag_v, list_edge_v = morpho_e_njv(corpus)\n",
    "            dict_total['sentence'].extend(list_sentence_e)\n",
    "            dict_total['sep_all'].extend(list_bag_a)\n",
    "            dict_total['sep_nj'].extend(list_bag_e)\n",
    "            dict_total['sep_njv'].extend(list_bag_v)\n",
    "            dict_total['edge_all'].extend(list_edge_a)\n",
    "            dict_total['edge_nj'].extend(list_edge_e)\n",
    "            dict_total['edge_njv'].extend(list_edge_v)\n",
    "        except UnicodeDecodeError:\n",
    "            print 'error'\n",
    "    Filer.writedump(dict_total, outputpath)\n",
    "    print outputpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpinosisDataset.goldの形態素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_path = glob.glob('./files/OpinosisDataset1.0/summaries-gold/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./files/OpinosisDataset1.0/summaries-gold/bathroom_bestwestern_hotel_sfo',\n",
       " './files/OpinosisDataset1.0/summaries-gold/staff_swissotel_chicago',\n",
       " './files/OpinosisDataset1.0/summaries-gold/navigation_amazon_kindle',\n",
       " './files/OpinosisDataset1.0/summaries-gold/performance_netbook_1005ha',\n",
       " './files/OpinosisDataset1.0/summaries-gold/free_bestwestern_hotel_sfo',\n",
       " './files/OpinosisDataset1.0/summaries-gold/speed_windows7',\n",
       " './files/OpinosisDataset1.0/summaries-gold/eyesight-issues_amazon_kindle',\n",
       " './files/OpinosisDataset1.0/summaries-gold/staff_bestwestern_hotel_sfo',\n",
       " './files/OpinosisDataset1.0/summaries-gold/sound_ipod_nano_8gb',\n",
       " './files/OpinosisDataset1.0/summaries-gold/screen_netbook_1005ha',\n",
       " './files/OpinosisDataset1.0/summaries-gold/features_windows7',\n",
       " './files/OpinosisDataset1.0/summaries-gold/fonts_amazon_kindle',\n",
       " './files/OpinosisDataset1.0/summaries-gold/interior_toyota_camry_2007',\n",
       " './files/OpinosisDataset1.0/summaries-gold/performance_honda_accord_2008',\n",
       " './files/OpinosisDataset1.0/summaries-gold/service_bestwestern_hotel_sfo',\n",
       " './files/OpinosisDataset1.0/summaries-gold/location_holiday_inn_london',\n",
       " './files/OpinosisDataset1.0/summaries-gold/service_swissotel_hotel_chicago',\n",
       " './files/OpinosisDataset1.0/summaries-gold/directions_garmin_nuvi_255W_gps',\n",
       " './files/OpinosisDataset1.0/summaries-gold/buttons_amazon_kindle',\n",
       " './files/OpinosisDataset1.0/summaries-gold/price_holiday_inn_london',\n",
       " './files/OpinosisDataset1.0/summaries-gold/food_holiday_inn_london',\n",
       " './files/OpinosisDataset1.0/summaries-gold/battery-life_amazon_kindle',\n",
       " './files/OpinosisDataset1.0/summaries-gold/location_bestwestern_hotel_sfo',\n",
       " './files/OpinosisDataset1.0/summaries-gold/updates_garmin_nuvi_255W_gps',\n",
       " './files/OpinosisDataset1.0/summaries-gold/accuracy_garmin_nuvi_255W_gps',\n",
       " './files/OpinosisDataset1.0/summaries-gold/battery-life_ipod_nano_8gb',\n",
       " './files/OpinosisDataset1.0/summaries-gold/display_garmin_nuvi_255W_gps',\n",
       " './files/OpinosisDataset1.0/summaries-gold/food_swissotel_chicago',\n",
       " './files/OpinosisDataset1.0/summaries-gold/voice_garmin_nuvi_255W_gps',\n",
       " './files/OpinosisDataset1.0/summaries-gold/transmission_toyota_camry_2007',\n",
       " './files/OpinosisDataset1.0/summaries-gold/rooms_bestwestern_hotel_sfo',\n",
       " './files/OpinosisDataset1.0/summaries-gold/keyboard_netbook_1005ha',\n",
       " './files/OpinosisDataset1.0/summaries-gold/seats_honda_accord_2008',\n",
       " './files/OpinosisDataset1.0/summaries-gold/gas_mileage_toyota_camry_2007',\n",
       " './files/OpinosisDataset1.0/summaries-gold/parking_bestwestern_hotel_sfo',\n",
       " './files/OpinosisDataset1.0/summaries-gold/screen_ipod_nano_8gb',\n",
       " './files/OpinosisDataset1.0/summaries-gold/interior_honda_accord_2008',\n",
       " './files/OpinosisDataset1.0/summaries-gold/size_asus_netbook_1005ha',\n",
       " './files/OpinosisDataset1.0/summaries-gold/video_ipod_nano_8gb',\n",
       " './files/OpinosisDataset1.0/summaries-gold/mileage_honda_accord_2008',\n",
       " './files/OpinosisDataset1.0/summaries-gold/screen_garmin_nuvi_255W_gps',\n",
       " './files/OpinosisDataset1.0/summaries-gold/comfort_honda_accord_2008',\n",
       " './files/OpinosisDataset1.0/summaries-gold/price_amazon_kindle',\n",
       " './files/OpinosisDataset1.0/summaries-gold/battery-life_netbook_1005ha',\n",
       " './files/OpinosisDataset1.0/summaries-gold/quality_toyota_camry_2007',\n",
       " './files/OpinosisDataset1.0/summaries-gold/service_holiday_inn_london',\n",
       " './files/OpinosisDataset1.0/summaries-gold/satellite_garmin_nuvi_255W_gps',\n",
       " './files/OpinosisDataset1.0/summaries-gold/rooms_swissotel_chicago',\n",
       " './files/OpinosisDataset1.0/summaries-gold/speed_garmin_nuvi_255W_gps',\n",
       " './files/OpinosisDataset1.0/summaries-gold/comfort_toyota_camry_2007',\n",
       " './files/OpinosisDataset1.0/summaries-gold/room_holiday_inn_london']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CoreNLP_PyWrapper:Starting java subprocess, and waiting for signal it's ready, with command: exec java -Xmx4g -XX:ParallelGCThreads=1 -cp '/home/ikegami/ikegami/lib/python2.7/site-packages/stanford_corenlp_pywrapper/lib/*:/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*'      corenlp.SocketServer --outpipe /tmp/corenlp_pywrap_pipe_pypid=12569_time=1472184282.29  --configdict '{\"annotators\": \"tokenize, ssplit, pos, lemma, parse\"}'\n",
      "INFO:CoreNLP_PyWrapper:Successful ping. The server has started.\n",
      "INFO:CoreNLP_PyWrapper:Subprocess is ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathroom_bestwestern_hotel_sfo\n",
      "staff_swissotel_chicago\n",
      "navigation_amazon_kindle\n",
      "performance_netbook_1005ha\n",
      "free_bestwestern_hotel_sfo\n",
      "speed_windows7\n",
      "eyesight-issues_amazon_kindle\n",
      "staff_bestwestern_hotel_sfo\n",
      "sound_ipod_nano_8gb\n",
      "screen_netbook_1005ha\n",
      "features_windows7\n",
      "fonts_amazon_kindle\n",
      "interior_toyota_camry_2007\n",
      "performance_honda_accord_2008\n",
      "service_bestwestern_hotel_sfo\n",
      "location_holiday_inn_london\n",
      "service_swissotel_hotel_chicago\n",
      "directions_garmin_nuvi_255W_gps\n",
      "buttons_amazon_kindle\n",
      "price_holiday_inn_london\n",
      "food_holiday_inn_london\n",
      "battery-life_amazon_kindle\n",
      "location_bestwestern_hotel_sfo\n",
      "updates_garmin_nuvi_255W_gps\n",
      "accuracy_garmin_nuvi_255W_gps\n",
      "battery-life_ipod_nano_8gb\n",
      "display_garmin_nuvi_255W_gps\n",
      "food_swissotel_chicago\n",
      "voice_garmin_nuvi_255W_gps\n",
      "transmission_toyota_camry_2007\n",
      "rooms_bestwestern_hotel_sfo\n",
      "keyboard_netbook_1005ha\n",
      "seats_honda_accord_2008\n",
      "gas_mileage_toyota_camry_2007\n",
      "parking_bestwestern_hotel_sfo\n",
      "screen_ipod_nano_8gb\n",
      "interior_honda_accord_2008\n",
      "size_asus_netbook_1005ha\n",
      "video_ipod_nano_8gb\n",
      "mileage_honda_accord_2008\n",
      "screen_garmin_nuvi_255W_gps\n",
      "comfort_honda_accord_2008\n",
      "price_amazon_kindle\n",
      "battery-life_netbook_1005ha\n",
      "quality_toyota_camry_2007\n",
      "service_holiday_inn_london\n",
      "satellite_garmin_nuvi_255W_gps\n",
      "rooms_swissotel_chicago\n",
      "speed_garmin_nuvi_255W_gps\n",
      "comfort_toyota_camry_2007\n",
      "room_holiday_inn_london\n"
     ]
    }
   ],
   "source": [
    "proc = CoreNLP(configdict={'annotators': 'tokenize, ssplit, pos, lemma, parse'},\n",
    "               corenlp_jars=[\"/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*\"])\n",
    "\n",
    "for path in list_path:\n",
    "    filename = path.replace('./files/OpinosisDataset1.0/summaries-gold/', '')\n",
    "    list_filepath = glob.glob(path+'/*')\n",
    "    dict_total = {'sentence':[],\n",
    "                  'sep_all':[]}\n",
    "    for path1 in list_filepath:\n",
    "        list_sentence = []\n",
    "        list_bag = []\n",
    "        list_text = Filer.readtxt(path1, LF='\\r\\n')\n",
    "        for sentence in list_text:\n",
    "            list_sentence_a, list_bag_a, _ = morpho_e(sentence)\n",
    "            list_sentence.extend(list_sentence_a)\n",
    "            list_bag.extend(list_bag_a)\n",
    "        dict_total['sentence'].append(list_sentence)\n",
    "        dict_total['sep_all'].append(list_bag)\n",
    "    print filename\n",
    "    Filer.writedump(dict_total, './files/OpinosisDataset1.0/preprocessed1/ans/'+filename+'.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_file = Filer.readdump('./files/OpinosisDataset1.0/preprocessed1/test/accuracy_garmin_nuvi_255W_gps.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ./files/OpinosisDataset1.0/preprocessed/test/eyesight-issues_amazon_kindle.dump\n",
      "all 437 433 True\n",
      "nj 234 197 True\n",
      "njv 315 314 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/food_swissotel_chicago.dump\n",
      "all 391 391 True\n",
      "nj 244 226 True\n",
      "njv 296 296 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/speed_windows7.dump\n",
      "all 546 542 True\n",
      "nj 313 274 True\n",
      "njv 396 391 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/features_windows7.dump\n",
      "all 414 411 True\n",
      "nj 227 198 True\n",
      "njv 305 302 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/display_garmin_nuvi_255W_gps.dump\n",
      "all 347 345 True\n",
      "nj 195 167 True\n",
      "njv 251 246 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/interior_toyota_camry_2007.dump\n",
      "all 527 524 True\n",
      "nj 334 301 True\n",
      "njv 410 405 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/video_ipod_nano_8gb.dump\n",
      "all 680 675 True\n",
      "nj 440 385 True\n",
      "njv 541 537 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/rooms_bestwestern_hotel_sfo.dump\n",
      "all 890 886 True\n",
      "nj 545 490 True\n",
      "njv 687 683 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/interior_honda_accord_2008.dump\n",
      "all 495 492 True\n",
      "nj 317 293 True\n",
      "njv 381 378 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/service_holiday_inn_london.dump\n",
      "all 764 761 True\n",
      "nj 487 435 True\n",
      "njv 592 588 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/staff_swissotel_chicago.dump\n",
      "all 705 702 True\n",
      "nj 442 400 True\n",
      "njv 552 544 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/screen_netbook_1005ha.dump\n",
      "all 834 833 True\n",
      "nj 516 466 True\n",
      "njv 657 653 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/location_holiday_inn_london.dump\n",
      "all 908 905 True\n",
      "nj 614 573 True\n",
      "njv 728 721 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/price_amazon_kindle.dump\n",
      "all 527 523 True\n",
      "nj 276 232 True\n",
      "njv 372 370 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/gas_mileage_toyota_camry_2007.dump\n",
      "all 493 491 True\n",
      "nj 278 247 True\n",
      "njv 346 343 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/price_holiday_inn_london.dump\n",
      "all 545 543 True\n",
      "nj 334 300 True\n",
      "njv 415 414 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/size_asus_netbook_1005ha.dump\n",
      "all 539 538 True\n",
      "nj 323 294 True\n",
      "njv 404 401 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/speed_garmin_nuvi_255W_gps.dump\n",
      "all 406 404 True\n",
      "nj 222 194 True\n",
      "njv 287 284 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/comfort_honda_accord_2008.dump\n",
      "all 617 614 True\n",
      "nj 385 352 True\n",
      "njv 472 469 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/voice_garmin_nuvi_255W_gps.dump\n",
      "all 451 450 True\n",
      "nj 262 220 True\n",
      "njv 337 335 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/staff_bestwestern_hotel_sfo.dump\n",
      "all 893 891 True\n",
      "nj 580 519 True\n",
      "njv 716 712 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/screen_garmin_nuvi_255W_gps.dump\n",
      "all 529 525 True\n",
      "nj 296 255 True\n",
      "njv 396 395 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/service_swissotel_hotel_chicago.dump\n",
      "all 857 853 True\n",
      "nj 534 471 True\n",
      "njv 671 664 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/battery-life_amazon_kindle.dump\n",
      "all 476 475 True\n",
      "nj 250 209 True\n",
      "njv 336 331 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/performance_netbook_1005ha.dump\n",
      "all 383 382 True\n",
      "nj 213 192 True\n",
      "njv 280 280 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/parking_bestwestern_hotel_sfo.dump\n",
      "all 454 451 True\n",
      "nj 245 207 True\n",
      "njv 320 316 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/performance_honda_accord_2008.dump\n",
      "all 309 308 True\n",
      "nj 173 160 True\n",
      "njv 216 213 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/battery-life_netbook_1005ha.dump\n",
      "all 1132 1128 True\n",
      "nj 716 643 True\n",
      "njv 907 901 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/location_bestwestern_hotel_sfo.dump\n",
      "all 788 785 True\n",
      "nj 540 504 True\n",
      "njv 631 626 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/keyboard_netbook_1005ha.dump\n",
      "all 518 517 True\n",
      "nj 311 279 True\n",
      "njv 385 384 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/service_bestwestern_hotel_sfo.dump\n",
      "all 646 643 True\n",
      "nj 403 366 True\n",
      "njv 505 502 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/buttons_amazon_kindle.dump\n",
      "all 686 682 True\n",
      "nj 363 317 True\n",
      "njv 512 507 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/satellite_garmin_nuvi_255W_gps.dump\n",
      "all 381 378 True\n",
      "nj 187 154 True\n",
      "njv 257 255 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/directions_garmin_nuvi_255W_gps.dump\n",
      "all 579 576 True\n",
      "nj 321 275 True\n",
      "njv 445 441 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/sound_ipod_nano_8gb.dump\n",
      "all 409 405 True\n",
      "nj 253 230 True\n",
      "njv 318 315 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/room_holiday_inn_london.dump\n",
      "all 1597 1593 True\n",
      "nj 1065 963 True\n",
      "njv 1316 1307 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/rooms_swissotel_chicago.dump\n",
      "all 668 664 True\n",
      "nj 402 362 True\n",
      "njv 500 492 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/quality_toyota_camry_2007.dump\n",
      "all 346 345 True\n",
      "nj 184 165 True\n",
      "njv 243 240 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/updates_garmin_nuvi_255W_gps.dump\n",
      "all 416 415 True\n",
      "nj 220 186 True\n",
      "njv 291 288 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/bathroom_bestwestern_hotel_sfo.dump\n",
      "all 399 397 True\n",
      "nj 223 195 True\n",
      "njv 288 286 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/seats_honda_accord_2008.dump\n",
      "all 422 417 True\n",
      "nj 253 226 True\n",
      "njv 309 306 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/mileage_honda_accord_2008.dump\n",
      "all 575 571 True\n",
      "nj 317 279 True\n",
      "njv 405 400 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/transmission_toyota_camry_2007.dump\n",
      "all 648 645 True\n",
      "nj 357 305 True\n",
      "njv 468 464 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/free_bestwestern_hotel_sfo.dump\n",
      "all 509 504 True\n",
      "nj 301 263 True\n",
      "njv 382 377 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/comfort_toyota_camry_2007.dump\n",
      "all 496 493 True\n",
      "nj 292 261 True\n",
      "njv 364 358 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/accuracy_garmin_nuvi_255W_gps.dump\n",
      "all 364 363 True\n",
      "nj 189 160 True\n",
      "njv 254 253 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/navigation_amazon_kindle.dump\n",
      "all 417 415 True\n",
      "nj 232 212 True\n",
      "njv 293 291 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/food_holiday_inn_london.dump\n",
      "all 547 543 True\n",
      "nj 358 331 True\n",
      "njv 428 426 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/battery-life_ipod_nano_8gb.dump\n",
      "all 339 338 True\n",
      "nj 194 169 True\n",
      "njv 249 249 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/screen_ipod_nano_8gb.dump\n",
      "all 354 353 True\n",
      "nj 220 204 True\n",
      "njv 264 263 True\n",
      "./files/OpinosisDataset1.0/preprocessed/test/fonts_amazon_kindle.dump\n",
      "all 423 420 True\n",
      "nj 234 205 True\n",
      "njv 308 307 True\n"
     ]
    }
   ],
   "source": [
    "filepath = glob.glob('./files/OpinosisDataset1.0/preprocessed/test/*')\n",
    "for path in filepath:\n",
    "    print path\n",
    "    dict_tmp = Filer.readdump(path)\n",
    "    list_word = set([word for row in dict_tmp['sep_all'] for word in row])\n",
    "    list_edge = set([word for row in dict_tmp['edge_all'] for words in row for word in words])\n",
    "    print 'all', len(list_word), len(list_edge), list_edge.issubset(list_word)\n",
    "    list_word = set([word for row in dict_tmp['sep_nj'] for word in row])\n",
    "    list_edge = set([word for row in dict_tmp['edge_nj'] for words in row for word in words])\n",
    "    print 'nj', len(list_word), len(list_edge), list_edge.issubset(list_word)\n",
    "    list_word = set([word for row in dict_tmp['sep_njv'] for word in row])\n",
    "    list_edge = set([word for row in dict_tmp['edge_njv'] for words in row for word in words])\n",
    "    print 'njv', len(list_word), len(list_edge), list_edge.issubset(list_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_tmp = Filer.readdump('./files/OpinosisDataset1.0/preprocessed/test/accuracy_garmin_nuvi_255W_gps.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./files/OpinosisDataset1.0/preprocessed/test/sound_ipod_nano_8gb.dump\n"
     ]
    }
   ],
   "source": [
    "filepath = glob.glob('./files/OpinosisDataset1.0/preprocessed/test/*')\n",
    "for path in filepath:\n",
    "    dict_tmp = Filer.readdump(path)\n",
    "    if len(dict_tmp['sentence']) == 101:\n",
    "        print path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### OpinosisDatasetの形態素解析\n",
    "1. トピックごとにdictを作成する\n",
    " * sentence: 元の文のリスト\n",
    " * sep_all: 含まれているすべての単語のリスト\n",
    " * sep_nj: 含まれている名詞と形容詞のリスト\n",
    " * sep_njv: 含まれている名詞と形容詞と動詞のリスト\n",
    " * pos_all: 含まれているすべての単語のpos\n",
    " * pos_nj: 含まれている名詞と形容詞のpos\n",
    " * pos_njv: 含まれている名詞と形容詞と動詞のpos\n",
    " * edge_nj: すべての単語の構文によるエッジ\n",
    " * edge_nj: 名詞と形容詞の構文によるエッジ\n",
    " * edge_njv: 名詞と形容詞と動詞の構文によるエッジ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_path = glob.glob('./files/OpinosisDataset1.0/topics/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CoreNLP_PyWrapper:Starting java subprocess, and waiting for signal it's ready, with command: exec java -Xmx4g -XX:ParallelGCThreads=1 -cp '/home/ikegami/ikegami/lib/python2.7/site-packages/stanford_corenlp_pywrapper/lib/*:/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*'      corenlp.SocketServer --outpipe /tmp/corenlp_pywrap_pipe_pypid=31807_time=1472454569.89  --configdict '{\"annotators\": \"tokenize, ssplit, pos, lemma, parse\"}'\n",
      "INFO:CoreNLP_PyWrapper:Successful ping. The server has started.\n",
      "INFO:CoreNLP_PyWrapper:Subprocess is ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./files/OpinosisDataset1.0/preprocessed2/gas_mileage_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/location_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/interior_honda_accord_2008.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/sound_ipod_nano_8gb.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/size_asus_netbook_1005ha.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/eyesight-issues_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/display_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/video_ipod_nano_8gb.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/buttons_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/screen_ipod_nano_8gb.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/location_holiday_inn_london.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/comfort_honda_accord_2008.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/service_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/features_windows7.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/fonts_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/bathroom_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/mileage_honda_accord_2008.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/screen_netbook_1005ha.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/speed_windows7.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/interior_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/staff_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/seats_honda_accord_2008.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/food_swissotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/quality_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/parking_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/voice_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/battery-life_ipod_nano_8gb.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/service_swissotel_hotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/screen_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/directions_garmin_nuvi_255W_gps.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/free_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/speed_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/navigation_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/satellite_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/performance_netbook_1005ha.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/price_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/transmission_toyota_camry_2007.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/comfort_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/food_holiday_inn_london.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/updates_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/accuracy_garmin_nuvi_255W_gps.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/price_holiday_inn_london.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/service_holiday_inn_london.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/battery-life_netbook_1005ha.dump\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/rooms_swissotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/keyboard_netbook_1005ha.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/battery-life_amazon_kindle.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/room_holiday_inn_london.dump\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/rooms_bestwestern_hotel_sfo.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed2/staff_swissotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed2/performance_honda_accord_2008.dump\n"
     ]
    }
   ],
   "source": [
    "proc = CoreNLP(configdict={'annotators': 'tokenize, ssplit, pos, lemma, parse'},\n",
    "               corenlp_jars=[\"/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*\"])\n",
    "\n",
    "for path in list_path:\n",
    "    dict_total = {'sentence':[],\n",
    "                  'sep_all':[],\n",
    "                  'sep_nj':[],\n",
    "                  'sep_njv':[],\n",
    "                  'pos_all':[],\n",
    "                  'pos_nj':[],\n",
    "                  'pos_njv':[],\n",
    "                  'edge_all': [],\n",
    "                  'edge_nj':[],\n",
    "                  'edge_njv': []}\n",
    "    filename = path.replace('./files/OpinosisDataset1.0/topics/', '').replace('.txt.data', '')\n",
    "    outputpath = './files/OpinosisDataset1.0/preprocessed3/'+filename+'.dump'\n",
    "    list_corpus = Filer.readtxt(path, LF='\\r\\n')\n",
    "    for corpus in list_corpus:\n",
    "        try:\n",
    "            list_sentence_a, list_bag_a, list_edge_a, list_pos_a = morpho_e_pos(corpus)\n",
    "            list_sentence_e, list_bag_e, list_edge_e, list_pos_e = morpho_e_nj_pos(corpus)\n",
    "            list_sentence_v, list_bag_v, list_edge_v, list_pos_v = morpho_e_njv_pos(corpus)\n",
    "            dict_total['sentence'].extend(list_sentence_e)\n",
    "            dict_total['sep_all'].extend(list_bag_a)\n",
    "            dict_total['sep_nj'].extend(list_bag_e)\n",
    "            dict_total['sep_njv'].extend(list_bag_v)\n",
    "            dict_total['pos_all'].extend(list_pos_a)\n",
    "            dict_total['pos_nj'].extend(list_pos_e)\n",
    "            dict_total['pos_njv'].extend(list_pos_v)\n",
    "            dict_total['edge_all'].extend(list_edge_a)\n",
    "            dict_total['edge_nj'].extend(list_edge_e)\n",
    "            dict_total['edge_njv'].extend(list_edge_v)\n",
    "        except UnicodeDecodeError:\n",
    "            print 'error'\n",
    "    Filer.writedump(dict_total, outputpath)\n",
    "    print outputpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
