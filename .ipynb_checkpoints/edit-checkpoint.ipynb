{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"\n",
    "ファイルの編集を色々とするためのファイル\n",
    "\"\"\"\n",
    "from filer2.filer2 import Filer\n",
    "import numpy as np\n",
    "import glob\n",
    "import collections\n",
    "import re\n",
    "import MeCab\n",
    "import itertools\n",
    "import CaboCha\n",
    "from stanford_corenlp_pywrapper import CoreNLP\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CoreNLP_PyWrapper:Starting java subprocess, and waiting for signal it's ready, with command: exec java -Xmx4g -XX:ParallelGCThreads=1 -cp '/home/ikegami/ikegami/lib/python2.7/site-packages/stanford_corenlp_pywrapper/lib/*:/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*'      corenlp.SocketServer --outpipe /tmp/corenlp_pywrap_pipe_pypid=15647_time=1471842274.84  --configdict '{\"annotators\": \"tokenize, ssplit, pos, lemma, parse\"}'\n",
      "INFO:CoreNLP_PyWrapper:Successful ping. The server has started.\n",
      "INFO:CoreNLP_PyWrapper:Subprocess is ready.\n"
     ]
    }
   ],
   "source": [
    "# 関数の定義\n",
    "mecab = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "proc = CoreNLP(configdict={'annotators': 'tokenize, ssplit, pos, lemma, parse'},\n",
    "               corenlp_jars=[\"/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*\"])\n",
    "\n",
    "def morpho(sentence):\n",
    "    res = mecab.parseToNode(sentence)\n",
    "    list_words = []\n",
    "    while res:\n",
    "        features = res.feature.split(\",\")\n",
    "        if (features[0] == \"名詞\" and features[1] in [\"一般\", \"固有名詞\", \"サ変接続\", \"形容動詞語幹\"]) or features[0] == \"形容詞\":\n",
    "            if features[6] == \"*\":\n",
    "                if res.surface != '':\n",
    "                    list_words.append(res.surface)\n",
    "            else:\n",
    "                if features[6] != '':\n",
    "                    list_words.append(features[6])\n",
    "        res = res.next\n",
    "    return list_words\n",
    "\n",
    "def morpho_v(sentence):\n",
    "    res = mecab.parseToNode(sentence)\n",
    "    list_words = []\n",
    "    while res:\n",
    "        features = res.feature.split(\",\")\n",
    "        if (features[0] == \"名詞\" and features[1] in [\"一般\", \"固有名詞\", \"サ変接続\", \"形容動詞語幹\"]) or features[0] == \"形容詞\" or (features[0] == \"動詞\" and features[1] == \"自立\"):\n",
    "            if features[6] == \"*\":\n",
    "                if res.surface != '':\n",
    "                    list_words.append(res.surface)\n",
    "            else:\n",
    "                if features[6] != '':\n",
    "                    list_words.append(features[6])\n",
    "        res = res.next\n",
    "    return list_words\n",
    "\n",
    "def morpho1(sentence):\n",
    "    res = mecab.parseToNode(sentence)\n",
    "    list_words = []\n",
    "    while res:\n",
    "        features = res.feature.split(\",\")\n",
    "        if features[0] != \"記号\":\n",
    "            if features[6] == \"*\":\n",
    "                if res.surface != '':\n",
    "                    list_words.append(res.surface)\n",
    "            else:\n",
    "                if features[6] != '':\n",
    "                    list_words.append(features[6])\n",
    "        res = res.next\n",
    "    return list_words\n",
    "\n",
    "cp = CaboCha.Parser('-f1 -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "\n",
    "def get_v_word(tree, chunk):\n",
    "    surface = []\n",
    "    for i in range(chunk.token_pos, chunk.token_pos + chunk.token_size):\n",
    "        token = tree.token(i)\n",
    "        features = token.feature.split(',')\n",
    "        if features[0] == '名詞' and features[1] in [\"一般\", \"固有名詞\", \"サ変接続\", \"形容動詞語幹\"]:\n",
    "            surface.append(token.surface)\n",
    "        elif features[0] == '形容詞':\n",
    "            surface.append(features[6])\n",
    "        elif features[0] == '動詞' and features[1] == '自立':\n",
    "            surface.append(features[6])\n",
    "    return tuple(surface)\n",
    "\n",
    "def get_v_words(line):\n",
    "    cp = CaboCha.Parser('-f1 -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "    tree = cp.parse(line)\n",
    "    chunk_dic = {}\n",
    "    chunk_id = 0\n",
    "    for i in range(0, tree.size()):\n",
    "        token = tree.token(i)\n",
    "        if token.chunk:\n",
    "            chunk_dic[chunk_id] = token.chunk\n",
    "            chunk_id += 1\n",
    "\n",
    "    tuples = []\n",
    "    for chunk_id, chunk in chunk_dic.items():\n",
    "        if chunk.link > 0:\n",
    "            from_surface =  get_v_word(tree, chunk)\n",
    "            to_chunk = chunk_dic[chunk.link]\n",
    "            to_surface = get_v_word(tree, to_chunk)\n",
    "            tuples.extend(itertools.product(from_surface, to_surface))\n",
    "    return tuples\n",
    "\n",
    "def get_word(tree, chunk):\n",
    "    surface = []\n",
    "    for i in range(chunk.token_pos, chunk.token_pos + chunk.token_size):\n",
    "        token = tree.token(i)\n",
    "        features = token.feature.split(',')\n",
    "        if features[0] == '名詞' and features[1] in [\"一般\", \"固有名詞\", \"サ変接続\", \"形容動詞語幹\"]:\n",
    "            surface.append(token.surface)\n",
    "        elif features[0] == '形容詞':\n",
    "            surface.append(features[6])\n",
    "    return tuple(surface)\n",
    "\n",
    "def get_words(line):\n",
    "    cp = CaboCha.Parser('-f1 -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "    tree = cp.parse(line)\n",
    "    chunk_dic = {}\n",
    "    chunk_id = 0\n",
    "    for i in range(0, tree.size()):\n",
    "        token = tree.token(i)\n",
    "        if token.chunk:\n",
    "            chunk_dic[chunk_id] = token.chunk\n",
    "            chunk_id += 1\n",
    "\n",
    "    tuples = []\n",
    "    for chunk_id, chunk in chunk_dic.items():\n",
    "        if chunk.link > 0:\n",
    "            from_surface =  get_word(tree, chunk)\n",
    "            to_chunk = chunk_dic[chunk.link]\n",
    "            to_surface = get_word(tree, to_chunk)\n",
    "            tuples.extend(itertools.product(from_surface, to_surface))\n",
    "    return tuples\n",
    "\n",
    "p_n = r'^NN'\n",
    "p_v = r'^VB'\n",
    "p_j = r'^JJ'\n",
    "\n",
    "def morpho_e_njv(sentence):\n",
    "    list_sentence = proc.parse_doc(sentence)['sentences']\n",
    "    list_all_bag = []\n",
    "    list_all_edge = []\n",
    "    list_all_sentence = []\n",
    "    list_all_lemmas = []\n",
    "    for dict_tmp in list_sentence:\n",
    "        sentence = [word for word in dict_tmp['tokens']\n",
    "                    if word != ',' and word != '.' and word != ' ']\n",
    "        list_all_sentence.append(' '.join(sentence))\n",
    "        list_lemmas = dict_tmp['lemmas']\n",
    "        list_deps = dict_tmp['deps_cc']\n",
    "        list_pos = dict_tmp['pos']\n",
    "        dict_key_lemmas = {}\n",
    "        dict_lemmas_key = {}\n",
    "        list_bag = []\n",
    "        for i, row in enumerate(zip(list_lemmas, list_pos)):\n",
    "            lemmas = row[0]\n",
    "            pos = row[1]\n",
    "\n",
    "            if re.match(p_n, pos) or re.match(p_j, pos) or re.match(p_v, pos):\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "            \n",
    "        list_edge = []\n",
    "        for deps in list_deps:\n",
    "            if deps[1] in dict_key_lemmas and deps[2] in dict_key_lemmas:\n",
    "                list_edge.append([dict_key_lemmas[deps[1]],\n",
    "                                  dict_key_lemmas[deps[2]]])\n",
    "\n",
    "        list_all_bag.append(list_bag)\n",
    "        list_all_edge.append(list_edge)\n",
    "    return list_all_sentence, list_all_bag, list_all_edge\n",
    "\n",
    "def morpho_e(sentence):\n",
    "    list_sentence = proc.parse_doc(sentence)['sentences']\n",
    "    list_all_bag = []\n",
    "    list_all_edge = []\n",
    "    for dict_tmp in list_sentence:\n",
    "        sentence = [word for word in dict_tmp['tokens']\n",
    "                    if word != ',' and word != '.' and word != ' ']\n",
    "        list_all_sentence.append(' '.join(sentence))\n",
    "        list_lemmas = dict_tmp['lemmas']\n",
    "        list_deps = dict_tmp['deps_cc']\n",
    "        list_pos = dict_tmp['pos']\n",
    "        dict_key_lemmas = {}\n",
    "        dict_lemmas_key = {}\n",
    "        list_bag = []\n",
    "        for i, row in enumerate(zip(list_lemmas, list_pos)):\n",
    "            lemmas = row[0]\n",
    "            pos = row[1]\n",
    "\n",
    "            if lemmas != ',' and lemmas != ' ' and lemmas != '.':\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "            \n",
    "        list_edge = []\n",
    "        for deps in list_deps:\n",
    "            if deps[1] in dict_key_lemmas and deps[2] in dict_key_lemmas:\n",
    "                list_edge.append([dict_key_lemmas[deps[1]],\n",
    "                                  dict_key_lemmas[deps[2]]])\n",
    "\n",
    "        list_all_bag.append(list_bag)\n",
    "        list_all_edge.append(list_edge)\n",
    "    return list_all_sentence, list_all_bag, list_all_edge\n",
    "\n",
    "def morpho_e_nj(sentence):\n",
    "    list_sentence = proc.parse_doc(sentence)['sentences']\n",
    "    list_all_bag = []\n",
    "    list_all_edge = []\n",
    "    list_all_sentence = []\n",
    "    for dict_tmp in list_sentence:\n",
    "        sentence = [word for word in dict_tmp['tokens']\n",
    "                    if word != ',' and word != '.' and word != ' ']\n",
    "        list_all_sentence.append(' '.join(sentence))\n",
    "        list_lemmas = dict_tmp['lemmas']\n",
    "        list_deps = dict_tmp['deps_cc']\n",
    "        list_pos = dict_tmp['pos']\n",
    "        dict_key_lemmas = {}\n",
    "        dict_lemmas_key = {}\n",
    "        list_bag = []\n",
    "        for i, row in enumerate(zip(list_lemmas, list_pos)):\n",
    "            lemmas = row[0]\n",
    "            pos = row[1]\n",
    "\n",
    "            if re.match(p_n, pos) or re.match(p_j, pos):\n",
    "                dict_key_lemmas[i] = lemmas.encode('utf-8')\n",
    "                dict_lemmas_key[lemmas.encode('utf-8')] = i\n",
    "                list_bag.append(lemmas.encode('utf-8'))\n",
    "            \n",
    "        list_edge = []\n",
    "        for deps in list_deps:\n",
    "            if deps[1] in dict_key_lemmas and deps[2] in dict_key_lemmas:\n",
    "                list_edge.append([dict_key_lemmas[deps[1]],\n",
    "                                  dict_key_lemmas[deps[2]]])\n",
    "\n",
    "        list_all_bag.append(list_bag)\n",
    "        list_all_edge.append(list_edge)\n",
    "    return list_all_sentence, list_all_bag, list_all_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_sentence, list_lemmas, list_bag, list_edge = morpho_e('The clusters of review sentences on the viewpoints from the products evaluation can be applied to various use. The topic models, for example Unigram Mixture (UM), can be used for this task.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'The', u'clusters', u'of', u'review', u'sentences', u'on', u'the', u'viewpoints', u'from', u'the', u'products', u'evaluation', u'can', u'be', u'applied', u'to', u'various', u'use'] [u'the', u'cluster', u'of', u'review', u'sentence', u'on', u'the', u'viewpoint', u'from', u'the', u'product', u'evaluation', u'can', u'be', u'apply', u'to', u'various', u'use'] ['cluster', 'review', 'sentence', 'viewpoint', 'product', 'evaluation', 'various', 'use'] [['cluster', 'sentence'], ['sentence', 'review'], ['sentence', 'viewpoint'], ['viewpoint', 'evaluation'], ['evaluation', 'product'], ['use', 'various']]\n",
      "[u'The', u'topic', u'models', u'for', u'example', u'Unigram', u'Mixture', u'-LRB-', u'UM', u'-RRB-', u'can', u'be', u'used', u'for', u'this', u'task'] [u'the', u'topic', u'model', u'for', u'example', u'Unigram', u'Mixture', u'-lrb-', u'UM', u'-rrb-', u'can', u'be', u'use', u'for', u'this', u'task'] ['topic', 'model', 'example', 'Unigram', 'Mixture', 'UM', 'task'] [['model', 'Mixture'], ['model', 'topic'], ['Mixture', 'example'], ['Mixture', 'Unigram'], ['Mixture', 'UM']]\n"
     ]
    }
   ],
   "source": [
    "for sentence, lemmas, bag, edge in zip(list_sentence, list_lemmas, list_bag, list_edge):\n",
    "    print sentence, lemmas, bag, edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全体のコーパスを施設別に集計する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = glob.glob('../topic_model/files/rakuten_corpus/rakuten_corpus_master/txtfile/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "list_file = [Filer.readtxt(path)  for path in list_filepath]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# タブで区切る\n",
    "list_file = [[sentence.split('\\t') for sentence in row] for row in list_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ホテル番号別に集計\n",
    "dict_hotel_sentence = collections.defaultdict(list)\n",
    "for row in list_file:\n",
    "    for sen in row:\n",
    "        dict_hotel_sentence[sen[0]].append(sen[1] + '\\t' + sen[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# review数が100件未満のホテルは除外して保存する\n",
    "for key, value in dict_hotel_sentence.items():\n",
    "    if len(value) >= 100:\n",
    "        Filer.writetxt(value, '../files/txtfile/review-'+key+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数を集計\n",
    "dict_count = {key: len(value) for key, value in dict_hotel_sentence.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 施設名と番号を合わせる\n",
    "list_name = Filer.readtxt('../files/travel03_hotelMaster_20160304.txt')\n",
    "list_name = [sen.split('\\t') for sen in list_name]\n",
    "dict_num_name = {row[0]: row[1] for row in list_name if len(row) == 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 年別にレビュー数を集計\n",
    "dict_counter = {}\n",
    "for key, value in dict_hotel_sentence.items():\n",
    "    dict_counter_tmp = collections.defaultdict(int)\n",
    "    for sen in value:\n",
    "        dict_counter_tmp[sen[0:4]] += 1\n",
    "    dict_counter[key] = dict_counter_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'2001': 3,\n",
       "             '2002': 6,\n",
       "             '2003': 17,\n",
       "             '2004': 6,\n",
       "             '2005': 54,\n",
       "             '2006': 288,\n",
       "             '2007': 96,\n",
       "             '2008': 42,\n",
       "             '2009': 38,\n",
       "             '2010': 31,\n",
       "             '2011': 11})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_count['9680']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 施設番号、施設名、レビュー数のファイルを作成する\n",
    "list_master = [['施設番号', '施設名', 'レビュー数',\n",
    "                '1999', '2000', '2001', '2002',\n",
    "                '2003', '2004', '2005', '2006',\n",
    "                '2007', '2008', '2009', '2010',\n",
    "                '2011']]\n",
    "for key, value in sorted(dict_count.items(), key=lambda x:x[1], reverse=True):\n",
    "    if key in dict_num_name:\n",
    "        list_master.append([key, dict_num_name[key], value, dict_counter[key]['1999'],\n",
    "                            dict_counter[key]['2000'], dict_counter[key]['2001'], dict_counter[key]['2002'],\n",
    "                            dict_counter[key]['2003'], dict_counter[key]['2004'], dict_counter[key]['2005'],\n",
    "                            dict_counter[key]['2006'], dict_counter[key]['2007'], dict_counter[key]['2008'],\n",
    "                            dict_counter[key]['2009'], dict_counter[key]['2010'], dict_counter[key]['2011']])\n",
    "    else:\n",
    "        list_master.append([key, '', value, dict_counter[key]['1999'],\n",
    "                            dict_counter[key]['2000'], dict_counter[key]['2001'], dict_counter[key]['2002'],\n",
    "                            dict_counter[key]['2003'], dict_counter[key]['2004'], dict_counter[key]['2005'],\n",
    "                            dict_counter[key]['2006'], dict_counter[key]['2007'], dict_counter[key]['2008'],\n",
    "                            dict_counter[key]['2009'], dict_counter[key]['2010'], dict_counter[key]['2011']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# csvfileとして保存\n",
    "Filer.writecsv(list_master, '../files/txtfile/master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析対象とするホテルのみを取り出して、sentenceごとのファイルにする\n",
    "* 51637：アパホテル＆リゾート＜東京ベイ幕張＞\n",
    "* 1238：オリエンタルホテル東京ベイ\n",
    "* 53171：加賀の湧泉　ドーミーイン金沢\n",
    "* 19191：新横浜プリンスホテル\n",
    "* 76887：ホテルモントレ　グラスミア大阪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = ['../files/txtfile/review-51637.txt',\n",
    "                 '../files/txtfile/review-1238.txt',\n",
    "                 '../files/txtfile/review-53171.txt',\n",
    "                 '../files/txtfile/review-19191.txt',\n",
    "                 '../files/txtfile/review-76887.txt',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2010年、2011年の文章のみを取得\n",
    "list_review_rev = []\n",
    "for path in list_filepath:\n",
    "    list_review = Filer.readtxt(path)\n",
    "    list_tmp = []\n",
    "    for review in list_review:\n",
    "        if review[0:4] == '2010' or review[0:4] == '2011':\n",
    "            list_tmp.append(review.split('\\t')[1])\n",
    "    list_review_rev.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#【ご利用の宿泊プラン】~を削除\n",
    "pattern = r'【ご利用の宿泊プラン】.+$'\n",
    "list_review_rev1 = []\n",
    "for row in list_review_rev:\n",
    "    list_tmp = []\n",
    "    for sentence in row:\n",
    "        list_tmp.append(re.sub(pattern, '', sentence))\n",
    "    list_review_rev1.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sentenceに変更、その際、ユーザー番号を頭に振る\n",
    "list_sentence = []\n",
    "for row in list_review_rev1:\n",
    "    list_tmp = []\n",
    "    for i, review in enumerate(row):\n",
    "        list_s = re.split(r'。|！|？', review)\n",
    "        for sen in list_s:\n",
    "            list_tmp.append([i, sen])\n",
    "    list_sentence.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 文字の前後の空白を削除\n",
    "list_sentence_rev = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        sentence = sen[1].replace(' ', '')\n",
    "        if len(sentence) > 1:\n",
    "            list_tmp.append([sen[0], sentence])\n",
    "    list_sentence_rev.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 先頭が、」もしくは）で始まってる場合、前の文にくっつける\n",
    "list_sentence_rev1 = []\n",
    "for row in list_sentence_rev:\n",
    "    list_tmp = []\n",
    "    for i in range(len(row)-1):\n",
    "        if re.match(r'」|』|\\)|）', row[i][1]):\n",
    "            pass\n",
    "        elif re.match(r'」|』|\\)|）', row[i+1][1]):\n",
    "            list_tmp.append([row[i][0], row[i][1]+row[i+1][1]])\n",
    "        else:\n",
    "            list_tmp.append([row[i][0], row[i][1]])\n",
    "    else:\n",
    "        list_tmp.append([row[i+1][0], row[i+1][1]])\n",
    "        list_sentence_rev1.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Filer.writetsv(list_sentence_rev1[0], '../files/sentencefile/sentence-51637.tsv')\n",
    "Filer.writetsv(list_sentence_rev1[1], '../files/sentencefile/sentence-1238.tsv')\n",
    "Filer.writetsv(list_sentence_rev1[2], '../files/sentencefile/sentence-53171.tsv')\n",
    "Filer.writetsv(list_sentence_rev1[3], '../files/sentencefile/sentence-19191.tsv')\n",
    "Filer.writetsv(list_sentence_rev1[4], '../files/sentencefile/sentence-76887.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### sentenceごとのファイルを形態素解析済みのファイルに変更する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../files/sentencefile/sentence-76887.tsv', '../files/sentencefile/sentence-1238.tsv', '../files/sentencefile/sentence-53171.tsv', '../files/sentencefile/sentence-19191.tsv', '../files/sentencefile/sentence-51637.tsv']\n"
     ]
    }
   ],
   "source": [
    "list_path = glob.glob('../files/sentencefile/*')\n",
    "print list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentence = [Filer.readtsv(path) for path in list_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_morpho = [[[sen[1], morpho(sen[1])] for sen in row]for row in list_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writedump(list_morpho[4], '../files/morphofile/type1/morpho-51637.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_morpho1 = [[[sen[1], morpho1(sen[1])] for sen in row] for row in list_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writedump(list_morpho1[4], '../files/morphofile/type2/morpho-51637.dump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高頻度語、低頻度語を数え上げて削除したファイルを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_word = [word for row in list_morpho for sen in row for word in sen[1]]\n",
    "list_word1 = [word for row in list_morpho1 for sen in row for word in sen[1]]\n",
    "dict_word_freq = collections.Counter(list_word)\n",
    "dict_word_freq1 = collections.Counter(list_word1)\n",
    "list_set_word = []\n",
    "for row in list_morpho:\n",
    "    for sen in row:\n",
    "        list_set_word.extend(list(set(sen[1])))\n",
    "        \n",
    "list_set_word1 = []\n",
    "for row in list_morpho1:\n",
    "    for sen in row:\n",
    "        list_set_word1.extend(list(set(sen[1])))\n",
    "\n",
    "dict_document_freq = collections.Counter(list_set_word)\n",
    "dict_document_freq1 = collections.Counter(list_set_word1)\n",
    "print len(list_morpho[0])+len(list_morpho[1])+len(list_morpho[2])+len(list_morpho[3])+len(list_morpho[4])\n",
    "print len(list_morpho1[0])+len(list_morpho1[1])+len(list_morpho1[2])+len(list_morpho1[3])+len(list_morpho1[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "部屋 0.166852010265\n",
      "利用 0.136783575706\n",
      "ホテル 0.100427715997\n",
      "良い 0.0860992301112\n",
      "宿泊 0.0706159110351\n",
      "満足 0.0563301967494\n",
      "便利 0.047005988024\n",
      "朝食 0.0458083832335\n",
      "よい 0.0455089820359\n",
      "広い 0.0444824636441\n"
     ]
    }
   ],
   "source": [
    "# 頻度の確認、10%切りで良さそう\n",
    "for row in sorted(dict_document_freq.items(), key=lambda x:x[1], reverse=True)[0:10]:\n",
    "    print row[0], row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 除去語の記録、10%以上、2回以下\n",
    "list_remove = []\n",
    "for row in dict_document_freq.items():\n",
    "    if row[1] >= 0.1:\n",
    "        list_remove.append(row[0])\n",
    "for row in dict_word_freq.items():\n",
    "    if row[1] <= 2:\n",
    "        list_remove.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Filer.writetxt(list_remove, '../files/preprocessedfile/dict/remove_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../files/morphofile/type1/morpho-51637.dump',\n",
       " '../files/morphofile/type1/morpho-53171.dump',\n",
       " '../files/morphofile/type1/morpho-1238.dump',\n",
       " '../files/morphofile/type1/morpho-76887.dump',\n",
       " '../files/morphofile/type1/morpho-19191.dump']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_path = glob.glob('../files/morphofile/type1/*.dump')\n",
    "list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentence = [Filer.readdump(path) for path in list_path]\n",
    "list_sentence_rev = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        list_tmp_tmp = [word for word in sen[1] if word not in list_remove]\n",
    "        list_tmp.append([sen[0], list_tmp_tmp])\n",
    "    list_sentence_rev.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 総単語数が１以下の文を削除\n",
    "list_sentence_rev = [[sen for sen in row if len(sen[1]) >= 2] for row in list_sentence_rev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writedump(list_sentence_rev[4], '../files/preprocessedfile/type11/wordlist/preprocessed-19191.dump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エッジリストの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../files/preprocessedfile/type11/wordlist/preprocessed-19191.dump',\n",
       " '../files/preprocessedfile/type11/wordlist/preprocessed-76887.dump',\n",
       " '../files/preprocessedfile/type11/wordlist/preprocessed-1238.dump',\n",
       " '../files/preprocessedfile/type11/wordlist/preprocessed-51637.dump',\n",
       " '../files/preprocessedfile/type11/wordlist/preprocessed-53171.dump']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_path = glob.glob('../files/preprocessedfile/type11/wordlist/*')\n",
    "list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentence = [Filer.readdump(path) for path in list_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bigram\n",
    "list_bigram = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        for i in range(len(sen[1])-1):\n",
    "            list_tmp.append([sen[1][i], sen[1][i+1]])\n",
    "    list_bigram.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writetsv(list_bigram[4], '../files/preprocessedfile/type11/bigram/bigram-53171.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cor\n",
    "list_cor = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        list_tmp.extend(list(itertools.combinations(tuple(sen[1]),2)))\n",
    "    list_tmp = [list(row) for row in list_tmp]\n",
    "    list_cor.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writetsv(list_cor[4], '../files/preprocessedfile/type11/cor/cor-53171.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'b'),\n",
       " ('a', 'c'),\n",
       " ('a', 'd'),\n",
       " ('a', 'e'),\n",
       " ('b', 'c'),\n",
       " ('b', 'd'),\n",
       " ('b', 'e'),\n",
       " ('c', 'd'),\n",
       " ('c', 'e'),\n",
       " ('d', 'e')]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = ['a', 'b', 'c', 'd', 'e']\n",
    "list(itertools.combinations(tuple(seq),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### PRTMによって、分類した後の文書を形態素解析する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_path = glob.glob('./files/classifiedfile/type11/bigram/sentence/*')\n",
    "removeword = './files/classifiedfile/type11/bigram/sentence/classified'\n",
    "list_name = [path.replace(removeword, '') for path in list_path]\n",
    "list_name = [name.replace('.txt', '') for name in list_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "list_sentence = [Filer.readtxt(path) for path in list_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 形態素解析\n",
    "list_master = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = [morpho_v(sentence) for sentence in row]\n",
    "    list_master.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "for row, name in zip(list_master, list_name):\n",
    "    Filer.writetsv(row, './files/classifiedfile/type11/bigram/morpho/n_adj_verb/classified'+name+'.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# エッジリストの作成\n",
    "list_bigram = []\n",
    "for row in list_master:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        for i in range(len(sen)-1):\n",
    "            list_tmp.append([sen[i], sen[i+1]])\n",
    "    list_bigram.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "for row, name in zip(list_bigram, list_name):\n",
    "    Filer.writetsv(row, './classifiedfile/type11/bigram/bigram/classified'+name+'.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cor\n",
    "list_cor = []\n",
    "for row in list_master:\n",
    "    list_tmp = []\n",
    "    for sen in row:\n",
    "        list_tmp.extend(list(itertools.combinations(tuple(sen),2)))\n",
    "    list_tmp = [list(row) for row in list_tmp]\n",
    "    list_cor.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "for row, name in zip(list_cor, list_name):\n",
    "    Filer.writetsv(row, './classifiedfile/type11/bigram/cor/classified'+name+'.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 構文構造を利用してエッジリストを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_path = glob.glob('files/classifiedfile/type11/bigram/sentence/classified*')\n",
    "removeword = 'files/classifiedfile/type11/bigram/sentence/classified'\n",
    "list_name = [path.replace(removeword, '') for path in list_path]\n",
    "list_name = [name.replace('.txt', '') for name in list_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "list_sentence = [Filer.readtxt(path) for path in list_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 構文によるエッジリストの作成(動詞なし)\n",
    "list_master = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = [[t[0], t[1]] for sentence in row for t in get_words(sentence)]\n",
    "    list_master.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開業 => 新横浜\n",
      "開業 => プリンス\n",
      "駅 => 近い\n",
      "近い => 近い\n",
      "アリーナ => 近い\n",
      "近い => 利用\n",
      "新横浜 => イベント\n",
      "横浜アリーナ => ライヴ\n",
      "ライヴ => あり\n",
      "ライヴ => 利用\n"
     ]
    }
   ],
   "source": [
    "for row in list_master[0][0:10]:\n",
    "    print row[0], '=>', row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "for row, name in zip(list_master, list_name):\n",
    "    Filer.writetsv(row, './files/classifiedfile/type11/bigram/syntax/non_verb/classified'+name+'.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 構文によるエッジリストの作成(動詞あり)\n",
    "list_master = []\n",
    "for row in list_sentence:\n",
    "    list_tmp = [[t[0], t[1]] for sentence in row for t in get_v_words(sentence)]\n",
    "    list_master.append(list_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 保存\n",
    "for row, name in zip(list_master, list_name):\n",
    "    Filer.writetsv(row, './files/classifiedfile/type11/bigram/syntax/verb/classified'+name+'.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "フロント  =>  配置\n",
      "フロント  =>  欲しい\n",
      "人  =>  配置\n",
      "人  =>  欲しい\n"
     ]
    }
   ],
   "source": [
    "sentence = '時間的に混むのがわかったいるんだから、もう少しフロントに人を配置して欲しい'\n",
    "t = get_words(sentence)\n",
    "for row in t:\n",
    "    print row[0], ' => ', row[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### OpinosisDatasetの基礎集計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "list_path_corpus = glob.glob('./files/OpinosisDataset1.0/topics/*')\n",
    "list_path_gold = glob.glob('./files/OpinosisDataset1.0/summaries-gold/*')\n",
    "print len(list_path_corpus)\n",
    "print len(list_path_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Garmin seems to be generally very accurate.',\n",
       " \"It's easy to use with an intuitive interface.\"]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_corpus_gold = Filer.readtxt('./files/OpinosisDataset1.0/summaries-gold/accuracy_garmin_nuvi_255W_gps/accuracy_garmin_nuvi_255W_gps.2.gold', LF='\\r\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpinosisDatasetの形態素解析\n",
    "1. トピックごとにdictを作成する\n",
    "* sentence: 元の文のリスト\n",
    "* sep_all: 含まれているすべての単語のリスト\n",
    "* sep_nj: 含まれている名詞と形容詞のリスト\n",
    "* sep_njv: 含まれている名詞と形容詞と動詞のリスト\n",
    "* edge_nj: すべての単語の構文によるエッジ\n",
    "* edge_nj: 名詞と形容詞の構文によるエッジ\n",
    "* edge_njv: 名詞と形容詞と動詞の構文によるエッジ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_path = glob.glob('./files/OpinosisDataset1.0/topics/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CoreNLP_PyWrapper:Starting java subprocess, and waiting for signal it's ready, with command: exec java -Xmx4g -XX:ParallelGCThreads=1 -cp '/home/ikegami/ikegami/lib/python2.7/site-packages/stanford_corenlp_pywrapper/lib/*:/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*'      corenlp.SocketServer --outpipe /tmp/corenlp_pywrap_pipe_pypid=15647_time=1471842311.58  --configdict '{\"annotators\": \"tokenize, ssplit, pos, lemma, parse\"}'\n",
      "INFO:CoreNLP_PyWrapper:Successful ping. The server has started.\n",
      "INFO:CoreNLP_PyWrapper:Subprocess is ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./files/OpinosisDataset1.0/preprocessed/gas_mileage_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/location_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/interior_honda_accord_2008.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/sound_ipod_nano_8gb.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/size_asus_netbook_1005ha.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/eyesight-issues_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/display_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/video_ipod_nano_8gb.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/buttons_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/screen_ipod_nano_8gb.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/location_holiday_inn_london.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/comfort_honda_accord_2008.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/service_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/features_windows7.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/fonts_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/bathroom_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/mileage_honda_accord_2008.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/screen_netbook_1005ha.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/speed_windows7.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/interior_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/staff_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/seats_honda_accord_2008.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/food_swissotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/quality_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/parking_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/voice_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/battery-life_ipod_nano_8gb.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/service_swissotel_hotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/screen_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/directions_garmin_nuvi_255W_gps.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/free_bestwestern_hotel_sfo.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/speed_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/navigation_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/satellite_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/performance_netbook_1005ha.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/price_amazon_kindle.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/transmission_toyota_camry_2007.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/comfort_toyota_camry_2007.dump\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/food_holiday_inn_london.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/updates_garmin_nuvi_255W_gps.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/accuracy_garmin_nuvi_255W_gps.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/price_holiday_inn_london.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/service_holiday_inn_london.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/battery-life_netbook_1005ha.dump\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/rooms_swissotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/keyboard_netbook_1005ha.dump\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/battery-life_amazon_kindle.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/room_holiday_inn_london.dump\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/rooms_bestwestern_hotel_sfo.dump\n",
      "error\n",
      "error\n",
      "error\n",
      "./files/OpinosisDataset1.0/preprocessed/staff_swissotel_chicago.dump\n",
      "./files/OpinosisDataset1.0/preprocessed/performance_honda_accord_2008.dump\n"
     ]
    }
   ],
   "source": [
    "proc = CoreNLP(configdict={'annotators': 'tokenize, ssplit, pos, lemma, parse'},\n",
    "               corenlp_jars=[\"/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*\"])\n",
    "\n",
    "for path in list_path:\n",
    "    dict_total = {'sentence':[],\n",
    "                  'sep_all':[],\n",
    "                  'sep_nj':[],\n",
    "                  'sep_njv':[],\n",
    "                  'edge_all': [],\n",
    "                  'edge_nj':[],\n",
    "                  'edge_njv': []}\n",
    "    filename = path.replace('./files/OpinosisDataset1.0/topics/', '').replace('.txt.data', '')\n",
    "    outputpath = './files/OpinosisDataset1.0/preprocessed/'+filename+'.dump'\n",
    "    list_corpus = Filer.readtxt(path, LF='\\r\\n')\n",
    "    for corpus in list_corpus:\n",
    "        try:\n",
    "            list_sentence_a, list_bag_a, list_edge_a = morpho_e(corpus)\n",
    "            list_sentence_e, list_bag_e, list_edge_e = morpho_e_nj(corpus)\n",
    "            list_sentence_v, list_bag_v, list_edge_v = morpho_e_njv(corpus)\n",
    "            dict_total['sentence'].extend(list_sentence_e)\n",
    "            dict_total['sep_all'].extend(list_bag_a)\n",
    "            dict_total['sep_nj'].extend(list_bag_e)\n",
    "            dict_total['sep_njv'].extend(list_bag_v)\n",
    "            dict_total['edge_all'].extend(list_edge_a)\n",
    "            dict_total['edge_nj'].extend(list_edge_e)\n",
    "            dict_total['edge_njv'].extend(list_edge_v)\n",
    "        except UnicodeDecodeError:\n",
    "            print 'error'\n",
    "    Filer.writedump(dict_total, outputpath)\n",
    "    print outputpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpinosisDataset.goldの形態素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_path = glob.glob('./files/OpinosisDataset1.0/summaries-gold/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_a = Filer.readtxt('./files/OpinosisDataset1.0/summaries-gold/accuracy_garmin_nuvi_255W_gps/accuracy_garmin_nuvi_255W_gps.1.gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This unit is generally quite accurate.  \\r',\n",
       " 'Set-up and usage are considered to be very easy. \\r',\n",
       " 'The maps can be updated, and tend to be reliable.\\r']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-227675db188c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdict_total\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mdict_total\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sep_all'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_bag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mFiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwritedump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_total\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./files/OpinosisDataset1.0/preprocessed/ans/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.dump'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/ikegami/ikegami/lib/python2.7/site-packages/filer2/filer2.pyc\u001b[0m in \u001b[0;36mwritedump\u001b[1;34m(arr, path, option)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwritedump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moption\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moption\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m     \u001b[0mPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mdump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPROTO\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mchr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDictionaryType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36m_batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    661\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m                 \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m                 \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    664\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSETITEM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave_list\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_appends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mListType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36m_batch_appends\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m                 \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAPPEND\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave_list\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_appends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mListType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36m_batch_appends\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m                 \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAPPEND\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, i, pack)\u001b[0m\n\u001b[0;32m    265\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mLONG_BINGET\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"<i\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mGET\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "proc = CoreNLP(configdict={'annotators': 'tokenize, ssplit, pos, lemma, parse'},\n",
    "               corenlp_jars=[\"/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*\"])\n",
    "\n",
    "for path in list_path:\n",
    "    filename = path.replace('./files/OpinosisDataset1.0/summaries-gold/', '')\n",
    "    list_filepath = glob.glob(path+'/*')\n",
    "    dict_total = {'sentence':[],\n",
    "                  'sep_all':[]}\n",
    "    for path1 in list_filepath:\n",
    "        list_sentence = []\n",
    "        list_bag = []\n",
    "        list_text = Filer.readtxt(path1, LF='\\r\\n')\n",
    "        for sentence in list_text:\n",
    "            list_sentence_a, list_bag_a, _ = morpho_e(sentence)\n",
    "            list_sentence.extend(list_sentence_a)\n",
    "            list_bag.extend(list_bag_a)\n",
    "        dict_total['sentence'].append(list_sentence)\n",
    "        dict_total['sep_all'].append(list_bag)\n",
    "    print filename\n",
    "    Filer.writedump(dict_total, './files/OpinosisDataset1.0/preprocessed/ans/'+filename+'.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
